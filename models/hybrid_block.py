"""
HybridBlock: å¹¶è¡ŒåŒåˆ†æ”¯æ··åˆæ¶æ„çš„æ ¸å¿ƒæ¨¡å—
åŒ…å«Mambaåˆ†æ”¯ï¼ˆè¯­ä¹‰ä¸»å¹²ï¼‰+ Attentionåˆ†æ”¯ï¼ˆå±€éƒ¨ç»†èŠ‚ï¼‰+ è½»é—¨æ§èåˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any, Tuple
import math
from torch.utils.checkpoint import checkpoint

from .mamba_block import MambaBlock
from .local_global_attn import LocalGlobalAttention

def _assert_finite(name, x):
    """æ—©æœŸå“¨å…µæ–­è¨€ï¼Œæ–¹ä¾¿å®šä½ NaN/Inf é—®é¢˜"""
    if not torch.isfinite(x).all():
        bad = (~torch.isfinite(x)).float().mean().item()
        raise RuntimeError(f"[NaN/Inf] {name}: ratio={bad:.6f}, shape={tuple(x.shape)}")


class SRTE(nn.Module):
    """Shared Relative Time Encoding: ç”Ÿæˆç»Ÿä¸€çš„ç›¸å¯¹æ—¶é—´/ä½ç½®ç¼–ç åŸºåº•"""
    
    def __init__(self, hidden_size: int, max_len: int = 65536, encoding_type: str = "learnable", factorized_rank: int = 0):
        super().__init__()
        self.hidden_size = hidden_size
        self.max_len = max_len
        self.encoding_type = encoding_type
        self.factorized_rank = factorized_rank
        
        if encoding_type == "learnable":
            if factorized_rank and factorized_rank < hidden_size:
                # ä½ç§©åˆ†è§£ï¼š[L, r] + [r, H] è€Œä¸æ˜¯ [L, H]
                self.lowrank = nn.Parameter(torch.randn(1, max_len, factorized_rank) * 0.02)
                self.proj = nn.Linear(factorized_rank, hidden_size, bias=False)
                print(f"SRTE using factorized learnable encoding: {max_len} x {factorized_rank} + {factorized_rank} x {hidden_size} = {max_len*factorized_rank + factorized_rank*hidden_size:,} params")
            else:
                # ä¼ ç»Ÿå…¨å‚æ•°å¯å­¦ä¹ ç¼–ç 
                self.freqs = nn.Parameter(torch.randn(1, max_len, hidden_size) * 0.02)
                print(f"SRTE using full learnable encoding: {max_len} x {hidden_size} = {max_len*hidden_size:,} params")
        elif encoding_type == "sincos":
            # æŒ‰éœ€è®¡ç®—sin/cosï¼šåªç¼“å­˜inv_freqï¼Œä¸é¢„å­˜æ•´è¡¨
            import math
            inv_freq = torch.exp(torch.arange(0, hidden_size, 2).float() * -(math.log(10000.0) / hidden_size))
            self.register_buffer('inv_freq', inv_freq)
            print(f"SRTE using on-the-fly sincos encoding: {len(inv_freq)} inv_freq params")
        else:
            raise ValueError(f"Unknown encoding_type: {encoding_type}")
    

    
    def forward(self, seq_len: int) -> torch.Tensor:
        """
        Args:
            seq_len: åºåˆ—é•¿åº¦
        Returns:
            ç›¸å¯¹æ—¶é—´ç¼–ç  [1, seq_len, hidden_size]
        """
        if seq_len > self.max_len:
            raise ValueError(f"Sequence length {seq_len} exceeds max_len {self.max_len}")
        
        if self.encoding_type == "learnable":
            if hasattr(self, 'lowrank'):
                # ä½ç§©åˆ†è§£ï¼šå…ˆå–lowrankéƒ¨åˆ†ï¼Œå†æŠ•å½±åˆ°full space
                return self.proj(self.lowrank[:, :seq_len, :])
            else:
                # ä¼ ç»Ÿlearnable
                return self.freqs[:, :seq_len, :]
        else:
            # sincosæŒ‰éœ€ç”Ÿæˆ
            device = self.inv_freq.device
            dtype = self.inv_freq.dtype
            t = torch.arange(seq_len, device=device, dtype=dtype).unsqueeze(1)  # [L, 1]
            freqs = t * self.inv_freq.unsqueeze(0)  # [L, H/2]
            emb = torch.zeros(seq_len, self.hidden_size, device=device, dtype=dtype)
            emb[:, 0::2] = torch.sin(freqs)
            emb[:, 1::2] = torch.cos(freqs)
            return emb.unsqueeze(0)  # [1, L, H]


class AlignmentMLP(nn.Module):
    """å¯¹é½æ¨¡å—ï¼šèåˆå‰å¯¹ä¸¤åˆ†æ”¯è¿›è¡Œç‰¹å¾å¯¹é½"""
    
    def __init__(self, hidden_size: int, intermediate_size: Optional[int] = None):
        super().__init__()
        if intermediate_size is None:
            intermediate_size = hidden_size
            
        self.net = nn.Sequential(
            nn.Linear(2 * hidden_size, intermediate_size),
            nn.GELU(),
            nn.Linear(intermediate_size, hidden_size)
        )
        # æ·»åŠ æ®‹å·®è¿æ¥å¸®åŠ©å­¦ä¹ 
        self.residual_proj = nn.Linear(2 * hidden_size, hidden_size)
        
    def forward(self, h_mamba: torch.Tensor, h_attn: torch.Tensor) -> torch.Tensor:
        """
        Args:
            h_mamba: Mambaåˆ†æ”¯è¾“å‡º [B, L, H]
            h_attn: Attentionåˆ†æ”¯è¾“å‡º [B, L, H]  
        Returns:
            å¯¹é½åçš„ç‰¹å¾ [B, L, H]
        """
        concatenated = torch.cat([h_mamba, h_attn], dim=-1)
        aligned = self.net(concatenated)
        residual = self.residual_proj(concatenated)
        return aligned + residual


class GateLowRank(nn.Module):
    """é€é€šé“é—¨æ§ï¼ˆä½ç§©å› å¼åˆ†è§£ä»¥çœå‚ï¼‰"""
    
    def __init__(self, hidden_size: int, rank: int = 256):
        super().__init__()
        self.hidden_size = hidden_size
        self.rank = min(rank, hidden_size // 2)  # ç¡®ä¿rankä¸ä¼šå¤ªå¤§
        
        self.u = nn.Linear(hidden_size, self.rank, bias=False)
        self.d = nn.Linear(self.rank, hidden_size, bias=False)
        self.dropout = nn.Dropout(0.1)
        
        # åˆå§‹åŒ–åå‘0.5
        nn.init.xavier_uniform_(self.u.weight)
        nn.init.xavier_uniform_(self.d.weight)
        
    def forward(self, z: torch.Tensor) -> torch.Tensor:
        """
        Args:
            z: è¾“å…¥ç‰¹å¾ [B, L, H]
        Returns:
            é—¨æ§æƒé‡ [B, L, H] âˆˆ (0,1)
        """
        intermediate = torch.tanh(self.u(z))
        intermediate = self.dropout(intermediate)
        gate_logits = self.d(intermediate)
        return torch.sigmoid(gate_logits)


class HybridBlock(nn.Module):
    """
    æ··åˆæ¶æ„çš„æ ¸å¿ƒæ¨¡å—ï¼š
    - å¹¶è¡ŒåŒåˆ†æ”¯ï¼šMambaï¼ˆé•¿ç¨‹è¯­ä¹‰ï¼‰+ Attentionï¼ˆå±€éƒ¨ç»†èŠ‚ï¼‰
    - è½»é—¨æ§èåˆï¼šé€é€šé“åŠ¨æ€æƒé‡
    - ç»Ÿä¸€æ—¶é—´ç¼–ç ï¼šSRTEä¸ºä¸¤åˆ†æ”¯æä¾›å¯¹é½çš„æ—¶é—´åŸºåº•
    """
    
    def __init__(
        self,
        hidden_size: int,
        num_heads: int = 32,
        window_size: int = 1024,
        global_heads: int = 2,
        gate_rank: int = 256,
        drop_branch_prob: float = 0.0,
        srte_encoding: str = "learnable",
        srte_max_len: int = 65536,
        srte_shared=None,
        srte_factorized_rank: int = 0,
        use_alignment: bool = True,
        shared_rope=None,
        gradient_checkpointing: bool = False
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.window_size = window_size
        self.global_heads = global_heads
        self.drop_branch_prob = drop_branch_prob
        self.use_alignment = use_alignment
        self.gradient_checkpointing = gradient_checkpointing
        
        # å±‚å½’ä¸€åŒ–
        self.ln_input = nn.LayerNorm(hidden_size)
        
        # ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯
        self.mamba = MambaBlock(hidden_size)
        self.attention = LocalGlobalAttention(
            hidden_size,
            num_heads=num_heads,
            window_size=window_size, 
            global_heads=global_heads,
            rope=shared_rope
        )
        
        # ç»Ÿä¸€æ—¶é—´ç¼–ç 
        if srte_shared is not None:
            # ä½¿ç”¨å…±äº«çš„SRTE
            self.srte = srte_shared
        else:
            # åˆ›å»ºç‹¬ç«‹çš„SRTE
            self.srte = SRTE(
                hidden_size, 
                max_len=srte_max_len, 
                encoding_type=srte_encoding,
                factorized_rank=srte_factorized_rank
            )
        
        # å¯¹é½æ¨¡å—
        if use_alignment:
            self.alignment = AlignmentMLP(hidden_size)
        
        # é—¨æ§èåˆ
        self.gate = GateLowRank(hidden_size, rank=gate_rank)
        self.fusion_proj = nn.Linear(hidden_size, hidden_size)
        
        # å°å‹MLP - ä½¿ç”¨2Hè€Œä¸æ˜¯4Hå‡å°‘å†…å­˜ä½¿ç”¨
        self.small_mlp = nn.Sequential(
            nn.Linear(hidden_size, 2 * hidden_size),
            nn.GELU(),
            nn.Linear(2 * hidden_size, hidden_size),
            nn.Dropout(0.1)
        )
        
        # è¾“å‡ºå±‚å½’ä¸€åŒ–
        self.ln_output = nn.LayerNorm(hidden_size)
        

        
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        kv_cache: Optional[Dict[str, torch.Tensor]] = None,
        mamba_state: Optional[torch.Tensor] = None,
        training: bool = True,
        collect_gate_stats: bool = False,
        use_cache: bool = False
    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]], Optional[torch.Tensor], Optional[Dict[str, Any]]]:
        """
        Args:
            hidden_states: è¾“å…¥éšè—çŠ¶æ€ [B, L, H]
            attention_mask: æ³¨æ„åŠ›æ©ç  [B, L] 
            kv_cache: KVç¼“å­˜å­—å…¸
            mamba_state: MambaçŠ¶æ€
            training: æ˜¯å¦è®­ç»ƒæ¨¡å¼
            collect_gate_stats: æ˜¯å¦æ”¶é›†é—¨æ§ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            output: è¾“å‡ºéšè—çŠ¶æ€ [B, L, H]
            new_kv_cache: æ›´æ–°åçš„KVç¼“å­˜
            new_mamba_state: æ›´æ–°åçš„MambaçŠ¶æ€
            auxiliary_outputs: è¾…åŠ©è¾“å‡ºï¼ˆé—¨æ§ç»Ÿè®¡ç­‰ï¼‰
        """
        batch_size, seq_len, _ = hidden_states.shape
        
        # è¾“å…¥å±‚å½’ä¸€åŒ–
        normalized_input = self.ln_input(hidden_states)
        if attention_mask is not None:
            normalized_input = normalized_input * attention_mask[:, :, None].to(normalized_input.dtype)
        
        # ğŸ” æ—©æœŸå“¨å…µæ£€æŸ¥
        _assert_finite("embeddings", hidden_states)
        _assert_finite("normalized_input", normalized_input)
        
        # è·å–ç»Ÿä¸€æ—¶é—´ç¼–ç 
        time_encoding = self.srte(seq_len).to(hidden_states.dtype)  # [1, L, H]
        
        # ä¸ºåˆ†æ”¯æ·»åŠ æ—¶é—´ç¼–ç ï¼ˆé¿å…ä¸RoPEå†²çªï¼Œåªç»™Mambaåˆ†æ”¯ï¼‰
        mamba_input = normalized_input + time_encoding
        attn_input = normalized_input  # Attentionåˆ†æ”¯ä½¿ç”¨RoPEï¼Œä¸åŠ SRTE
        
        # å¹¶è¡Œè®¡ç®—ä¸¤åˆ†æ”¯ï¼Œä½¿ç”¨é€‰æ‹©æ€§æ¢¯åº¦æ£€æŸ¥ç‚¹
        if self.gradient_checkpointing and training and torch.is_grad_enabled():
            # å¯¹Mambaåˆ†æ”¯ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆåªcheckpointè¾“å‡ºï¼Œstateä¸å‚ä¸ï¼‰
            def mamba_forward(x):
                return self.mamba(x, state=mamba_state)[0]  # åªè¿”å›è¾“å‡º
            
            h_mamba = checkpoint(mamba_forward, mamba_input, use_reentrant=False)
            # stateåœ¨è®­ç»ƒæ—¶é€šå¸¸ä¸éœ€è¦ï¼Œæ¨ç†æ—¶ä¼šèµ°elseåˆ†æ”¯
            new_mamba_state = None
            
            # å¯¹Attentionåˆ†æ”¯ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆåªcheckpointè¾“å‡ºï¼Œcacheä¸å‚ä¸ï¼‰
            def attn_forward(x):
                return self.attention(x, attention_mask=attention_mask, 
                                    kv_cache=kv_cache, use_cache=False)[0]  # åªè¿”å›è¾“å‡º
            
            h_attn = checkpoint(attn_forward, attn_input, use_reentrant=False)
            # cacheåœ¨è®­ç»ƒæ—¶é€šå¸¸ä¸éœ€è¦ï¼Œæ¨ç†æ—¶ä¼šèµ°elseåˆ†æ”¯
            new_kv_cache = None
        else:
            # æ­£å¸¸è®¡ç®—ï¼ˆæ¨ç†æ¨¡å¼æˆ–ä¸ä½¿ç”¨checkpointï¼‰
            h_mamba, new_mamba_state = self.mamba(mamba_input, state=mamba_state)
            h_attn, new_kv_cache = self.attention(
                attn_input, 
                attention_mask=attention_mask, 
                kv_cache=kv_cache,
                use_cache=use_cache
            )
        
        # BranchDropoutæ­£åˆ™åŒ–ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰- ç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒæ—¶æ‰€æœ‰rankä¸€è‡´
        if training and self.drop_branch_prob > 0:
            # ğŸ”§ åˆ†å¸ƒå¼å®‰å…¨çš„éšæœºåˆ†æ”¯ä¸¢å¼ƒ
            try:
                import torch.distributed as dist
                if dist.is_initialized():
                    # åˆ†å¸ƒå¼æ¨¡å¼ï¼šrank0ç”Ÿæˆéšæœºæ•°ï¼Œå¹¿æ’­ç»™æ‰€æœ‰rankç¡®ä¿ä¸€è‡´æ€§
                    if dist.get_rank() == 0:
                        drop_decision = torch.rand(2, device=h_mamba.device)  # [drop_prob, branch_choice]
                    else:
                        drop_decision = torch.zeros(2, device=h_mamba.device)
                    
                    # å¹¿æ’­éšæœºæ•°åˆ°æ‰€æœ‰rank
                    dist.broadcast(drop_decision, src=0)
                    
                    should_drop = drop_decision[0].item() < self.drop_branch_prob
                    drop_mamba = drop_decision[1].item() < 0.5
                else:
                    # éåˆ†å¸ƒå¼æ¨¡å¼ï¼šæ­£å¸¸éšæœº
                    should_drop = torch.rand(1).item() < self.drop_branch_prob
                    drop_mamba = torch.rand(1).item() < 0.5
                    
            except ImportError:
                # æœªå®‰è£…åˆ†å¸ƒå¼åŒ…ï¼šæ­£å¸¸éšæœº
                should_drop = torch.rand(1).item() < self.drop_branch_prob
                drop_mamba = torch.rand(1).item() < 0.5
                
            if should_drop:
                if drop_mamba:
                    h_mamba = torch.zeros_like(h_mamba)
                else:
                    h_attn = torch.zeros_like(h_attn)
        
        # ğŸ” å“¨å…µæ£€æŸ¥åˆ†æ”¯è¾“å‡º
        _assert_finite("mamba_out", h_mamba)
        _assert_finite("attn_out", h_attn)
        
        # ç‰¹å¾å¯¹é½
        if self.use_alignment:
            aligned_features = self.alignment(h_mamba, h_attn)
        else:
            aligned_features = (h_mamba + h_attn) / 2
        
        _assert_finite("aligned", aligned_features)
        
        # é—¨æ§èåˆ
        gate_weights = self.gate(aligned_features)  # [B, L, H]
        _assert_finite("gate", gate_weights)
        fused_features = gate_weights * h_mamba + (1 - gate_weights) * h_attn
        
        # æŠ•å½±å’Œå°å‹MLP
        projected = self.fusion_proj(fused_features)
        mlp_output = self.small_mlp(aligned_features)
        
        # æ®‹å·®è¿æ¥å’Œè¾“å‡ºå±‚å½’ä¸€åŒ–
        output = hidden_states + projected + mlp_output
        output = self.ln_output(output)
        
        # æ”¶é›†è¾…åŠ©ä¿¡æ¯
        auxiliary_outputs = None
        if collect_gate_stats:
            auxiliary_outputs = {
                'gate_weights': gate_weights,  # ä¿ç•™æ¢¯åº¦ï¼Œä¸è¦detach
                'gate_mean': gate_weights.mean().item(),
                'gate_std': gate_weights.std().item(),
                'mamba_norm': h_mamba.norm().item(),
                'attn_norm': h_attn.norm().item()
            }
        
        return output, new_kv_cache, new_mamba_state, auxiliary_outputs


def load_balance_loss(gate_weights: torch.Tensor, target: float = 0.5, margin: float = 0.1) -> torch.Tensor:
    """
    è´Ÿè½½å‡è¡¡æŸå¤±ï¼šçº¦æŸé—¨æ§æƒé‡çš„batchå‡å€¼æ¥è¿‘ç›®æ ‡å€¼
    
    Args:
        gate_weights: é—¨æ§æƒé‡ [B, L, H] âˆˆ (0,1)
        target: ç›®æ ‡å‡å€¼
        margin: å…è®¸çš„åå·®èŒƒå›´
        
    Returns:
        è´Ÿè½½å‡è¡¡æŸå¤±
    """
    mean_gate = gate_weights.mean()
    lower_bound = target - margin
    upper_bound = target + margin
    
    # Hinge lossåˆ°ç›®æ ‡åŒºé—´
    loss = F.relu(lower_bound - mean_gate) + F.relu(mean_gate - upper_bound)
    return loss


def gate_entropy_regularization(gate_weights: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
    """
    é—¨æ§ç†µæ­£åˆ™åŒ–ï¼šæƒ©ç½šæç«¯çš„0/1é—¨æ§ï¼Œé¼“åŠ±å¤šæ ·æ€§
    
    Args:
        gate_weights: é—¨æ§æƒé‡ [B, L, H] âˆˆ (0,1)
        eps: æ•°å€¼ç¨³å®šæ€§å‚æ•°
        
    Returns:
        ç†µæ­£åˆ™åŒ–æŸå¤±ï¼ˆè´Ÿç†µï¼Œå³æƒ©ç½šä½ç†µï¼‰
    """
    # æ•°å€¼ç¨³å®šæ€§å¤¹æ–­
    p = torch.clamp(gate_weights, eps, 1 - eps)
    
    # è®¡ç®—äºŒå…ƒç†µ
    entropy = -(p * torch.log(p) + (1 - p) * torch.log(1 - p))
    
    # è¿”å›è´Ÿç†µä½œä¸ºæ­£åˆ™é¡¹ï¼ˆæƒ©ç½šä½ç†µ/æç«¯å€¼ï¼‰
    return -entropy.mean() 