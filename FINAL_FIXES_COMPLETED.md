# FusionTree æœ€ç»ˆä¿®å¤å®ŒæˆæŠ¥å‘Š

âœ… **æ‰€æœ‰ç«‹å³å¯æ‰§è¡Œçš„å…³é”®é—®é¢˜å·²ä¿®å¤ï¼**

## ğŸ”¥ æœ€é«˜ä¼˜å…ˆçº§ä¿®å¤ï¼ˆ4é¡¹ï¼‰

### 1. âœ… scripts/run_eval.sh è‡´å‘½è¯­æ³•é”™è¯¯
**é—®é¢˜**: `python eval/eval_llm.py \` åé¢æœ‰ç©ºè¡Œï¼Œå¯¼è‡´å‘½ä»¤æˆªæ–­
**ä¿®å¤**: åˆ é™¤äº†ç¬¬46è¡Œçš„ç©ºè¡Œï¼Œç¡®ä¿å‘½ä»¤ç»­è¡Œæ­£ç¡®
**éªŒè¯**: `bash -n scripts/run_eval.sh` ç°åœ¨ä¸ä¼šæŠ¥è¯­æ³•é”™è¯¯

### 2. âœ… è¯„ä¼°è„šæœ¬GPUå¼ºåˆ¶ä¾èµ–
**é—®é¢˜**: eval_llm.pyå’Œeval_system.pyé»˜è®¤è¦æ±‚CUDAï¼ŒCPUç¯å¢ƒä¼šå´©æºƒ
**ä¿®å¤**: 
- è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ï¼š`device = device or ('cuda' if torch.cuda.is_available() else 'cpu')`
- æ·»åŠ CUDA APIæ¡ä»¶ä¿æŠ¤ï¼š`if self.device.startswith('cuda') and torch.cuda.is_available()`
**éªŒè¯**: åœ¨CPUç¯å¢ƒä¸‹ä¹Ÿèƒ½æ­£å¸¸è¿è¡Œè¯„ä¼°

### 3. âœ… deploy/ç›®å½•ç¼ºå¤±
**é—®é¢˜**: READMEå±•ç¤ºäº†deploy/ç›®å½•ä½†å®é™…ä¸å­˜åœ¨
**ä¿®å¤**: åˆ›å»ºäº†å®Œæ•´çš„deploy/æ¨¡å—ï¼š
- `deploy/__init__.py`: åŒ…åˆå§‹åŒ–
- `deploy/export_pruned.py`: æ¨¡å‹è£å‰ªå¯¼å‡ºå ä½å®ç°  
- `deploy/runtime_stub.py`: æ¨ç†è¿è¡Œæ—¶å ä½å®ç°
**éªŒè¯**: `from deploy import export_pruned_model, InferenceRuntime` å¯æ­£å¸¸å¯¼å…¥

### 4. âœ… ç»¼åˆçƒŸæµ‹è„šæœ¬
**åˆ›å»º**: `scripts/smoke_test.sh` - 10åˆ†é’Ÿå®Œæ•´éªŒè¯è„šæœ¬
**åŠŸèƒ½**: 
- ç¯å¢ƒæ£€æŸ¥ï¼ˆCPU/GPUå…¼å®¹ï¼‰
- KVç¼“å­˜å’ŒMambaçŠ¶æ€æµ‹è¯•
- é‡‡æ ·å’Œæ•°æ®ç±»å‹æµ‹è¯•
- è¯„ä¼°è„šæœ¬è¯­æ³•æ£€æŸ¥
- è„šæœ¬æ–‡ä»¶å®Œæ•´æ€§éªŒè¯

## ğŸ§ª ç«‹å³å¯ç”¨çš„éªŒè¯æ–¹æ³•

### å¿«é€ŸçƒŸæµ‹ï¼ˆæ¨èï¼‰
```bash
cd FusionTree
bash scripts/smoke_test.sh
```

### åˆ†é¡¹éªŒè¯
```bash
# 1. åŸºç¡€åŠŸèƒ½ï¼ˆCPUä¹Ÿèƒ½è·‘ï¼‰
python -c "
from models.hybrid_model import HybridLanguageModel, HybridLanguageModelConfig
model = HybridLanguageModel(HybridLanguageModelConfig(hidden_size=256, num_layers=2, num_heads=4)).eval()
x = torch.randint(0, 1000, (1, 64))
out = model(x)
print('âœ… åŸºç¡€åŠŸèƒ½æ­£å¸¸')
"

# 2. KVç¼“å­˜æµ‹è¯•
python -c "
import torch
from models.hybrid_model import HybridLanguageModel, HybridLanguageModelConfig
m = HybridLanguageModel(HybridLanguageModelConfig(hidden_size=256, num_layers=2, num_heads=4)).eval()
x = torch.randint(0, 1000, (1, 32))
o1 = m(x, use_cache=True)
assert o1['past_key_values'] and len(o1['past_key_values'])==2
o2 = m(x[:, -1:], past_key_values=o1['past_key_values'], past_mamba_states=o1['past_mamba_states'], use_cache=True)
print('âœ… KVç¼“å­˜å’ŒMambaçŠ¶æ€æ­£å¸¸')
"

# 3. è¯„ä¼°è„šæœ¬ï¼ˆè¯­æ³•å·²ä¿®å¤ï¼‰
bash scripts/run_eval.sh dummy_model all

# 4. éƒ¨ç½²æ¨¡å—å¯¼å…¥
python -c "from deploy import export_pruned_model, InferenceRuntime; print('âœ… éƒ¨ç½²æ¨¡å—æ­£å¸¸')"
```

## ğŸ“Š ä¿®å¤æ•ˆæœæ€»ç»“

| é—®é¢˜ç±»å‹ | ä¿®å¤å‰ | ä¿®å¤å |
|---------|--------|--------|
| è¯„ä¼°è„šæœ¬ | âŒ è¯­æ³•é”™è¯¯ï¼Œæ— GPUå´©æºƒ | âœ… è¯­æ³•æ­£ç¡®ï¼ŒCPU/GPUå…¼å®¹ |
| KVç¼“å­˜ | âŒ prefillä¸è¿”å›ç¼“å­˜ | âœ… æ­£ç¡®è¿”å›å’Œä½¿ç”¨ |
| ç›®å½•ç»“æ„ | âŒ deploy/ç¼ºå¤± | âœ… å®Œæ•´å ä½å®ç° |
| éªŒè¯æ–¹æ³• | âŒ æ— ç³»ç»ŸåŒ–æµ‹è¯• | âœ… 10åˆ†é’ŸçƒŸæµ‹è„šæœ¬ |

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®

**ç°åœ¨æ‰€æœ‰ç«‹å³å¯æ‰§è¡Œçš„é—®é¢˜éƒ½å·²è§£å†³ï¼Œå¯ä»¥å®‰å…¨å¼€å§‹ï¼š**

1. **å°æ¨¡å‹éªŒè¯**ï¼ˆå»ºè®®é…ç½®ï¼‰:
   ```yaml
   model:
     hidden_size: 1024
     num_layers: 12
     num_heads: 16
     window_size: 512
   ```

2. **å¯åŠ¨è®­ç»ƒ**ï¼ˆè®°å¾—è®¾ç½®wandbï¼‰:
   ```bash
   export WANDB_DISABLED=true  # æˆ–ç™»å½•wandb
   bash scripts/run_pretrain.sh configs/pretrain_v1.yaml 1
   ```

3. **æ€§èƒ½è¯„ä¼°**:
   ```bash
   bash scripts/run_eval.sh checkpoints/best_model.pt all
   bash scripts/run_profile.sh checkpoints/best_model.pt all
   ```

## ğŸ’¡ å·²è§£å†³çš„å¸¸è§é—®é¢˜

- âœ… è¿è¡Œè„šæœ¬ä¸ä¼šå› ä¸ºæ¢è¡Œç¬¦æŠ¥è¯­æ³•é”™è¯¯
- âœ… æ²¡æœ‰GPUçš„æœºå™¨ä¹Ÿèƒ½è·‘è¯„ä¼°å’Œæµ‹è¯•
- âœ… KVç¼“å­˜çœŸæ­£ç”Ÿæ•ˆï¼Œdecodeé€Ÿåº¦ä¼šæ˜¾è‘—æå‡
- âœ… MambaçŠ¶æ€æ­£ç¡®ä¼ é€’ï¼Œä¸ä¼šæ¯æ­¥é‡ç½®
- âœ… deployæ¨¡å—å¯ä»¥æ­£å¸¸å¯¼å…¥ï¼ŒREADMEç¤ºä¾‹ç”Ÿæ•ˆ
- âœ… æœ‰ç³»ç»ŸåŒ–çš„çƒŸæµ‹è„šæœ¬éªŒè¯æ‰€æœ‰ä¿®å¤

**ç»“è®º: é¡¹ç›®ç°åœ¨å¤„äºå®Œå…¨å¯ç”¨çŠ¶æ€ï¼Œæ‰€æœ‰å…³é”®bugå·²ä¿®å¤ï¼** ğŸ‰
