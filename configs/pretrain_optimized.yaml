# HybridLanguageModel å°å‹ç‰ˆæœ¬ - å†…å­˜ä¼˜åŒ–é…ç½®
# é¢„è®¡å‚æ•°é‡: ~3-4B (å¤§å¹…é™ä½å†…å­˜ä½¿ç”¨)

training:
  max_steps: 50         # å¿«é€Ÿæµ‹è¯•è®­ç»ƒæµç¨‹
  warmup_steps: 10      # ç›¸åº”å‡å°‘warmupæ­¥æ•°
  learning_rate: 0.0001  # 1e-4
  batch_size: 2          # ä¿å®ˆè®¾ç½®å‡å°‘å†…å­˜ä½¿ç”¨
  gradient_accumulation_steps: 4   # ä»16é™åˆ°4ï¼Œå‡å°‘å•stepæ‰€éœ€æ ·æœ¬æ•°
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 0.00000001  # 1e-8
  
  # å­¦ä¹ ç‡è°ƒåº¦
  lr_scheduler: "cosine"
  min_lr_ratio: 0.1
  
  # æ¢¯åº¦ç›¸å…³
  max_grad_norm: 1.0
  gradient_checkpointing: true
  
  # æ··åˆç²¾åº¦
  fp16: false
  bf16: true
  
  # æ­£åˆ™åŒ–ç³»æ•°
  load_balance_coeff: 0.1
  entropy_reg_coeff: 0.0001  # 1e-4
  distill_coeff: 0.0  # è®¾ä¸º > 0 å¯ç”¨è’¸é¦
  distill_temperature: 4.0
  
  # æ•°æ®ç›¸å…³
  max_seq_length: 32768  # å‡å°åºåˆ—é•¿åº¦ä»¥é…åˆSRTEä¼˜åŒ–
  
  # ä¿å­˜å’Œè¯„ä¼°
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  save_total_limit: 3

model:
  # åŸºç¡€æ¶æ„å‚æ•° - å°å‹ç‰ˆæœ¬é™ä½å†…å­˜ä½¿ç”¨
  vocab_size: 50432
  hidden_size: 2048         # ä»4096é™åˆ°2048ï¼Œå¤§å¹…å‡å°‘å‚æ•°é‡
  num_layers: 24            # ä»32é™åˆ°24å±‚
  num_heads: 16             # ä»32é™åˆ°16
  window_size: 1024
  global_heads: 2
  gate_rank: 128            # ä»256é™åˆ°128
  max_position_embeddings: 32768  # ä¸max_seq_lengthä¸€è‡´
  
  # å…³é”®ä¼˜åŒ–ï¼šSRTEé…ç½®
  srte_encoding: "learnable"        # ä½¿ç”¨å¯å­¦ä¹ ç¼–ç 
  srte_share_across_layers: true    # ğŸ”¥ å¯ç”¨å±‚é—´å…±äº«SRTEï¼ŒèŠ‚çœ8.3Bå‚æ•°
  srte_factorized_rank: 128         # ğŸ”¥ ä½¿ç”¨ä½ç§©åˆ†è§£ï¼Œè¿›ä¸€æ­¥å‹ç¼©å‚æ•°
  
  # æ³¨æ„åŠ›é…ç½®
  attention_type: "local_global"    # local_global æˆ– pyramidal
  
  # å…¶ä»–ä¼˜åŒ–
  tie_word_embeddings: true         # ğŸ”¥ ç»‘å®šè¯åµŒå…¥æƒé‡ï¼ŒèŠ‚çœ206Må‚æ•°
  use_alignment: true
  
  # æ­£åˆ™åŒ–
  layer_norm_eps: 0.00001  # 1e-5
  dropout: 0.1
  drop_branch_prob: 0.1  # âœ… å·²ä¿®å¤åˆ†å¸ƒå¼éšæœºæ€§é—®é¢˜ï¼Œå¯å®‰å…¨ä½¿ç”¨
  
  # æŸå¤±å‡½æ•°æƒé‡
  load_balance_coeff: 0.1
  entropy_reg_coeff: 0.0001  # 1e-4
  
  # æ¨ç†é…ç½®
  use_cache: true
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2

data:
  # æ•°æ®é›†é…ç½®
  train_data_paths:
    - "data/dummy_train.jsonl"
  
  eval_data_paths:
    longbench:
      - "data/dummy_eval.jsonl"
  
  # é¢„å¤„ç†
  tokenizer_path: "gpt2"  # ä½¿ç”¨gpt2 tokenizer
  max_length: 1024        # ä»32768é™åˆ°1024å‡å°‘å†…å­˜ä½¿ç”¨
  min_length: 1           # é™ä½æœ€å°é•¿åº¦é¿å…è¿‡æ»¤è¿‡å¤šæ ·æœ¬
  concat_docs: false       # å¯ç”¨æ–‡æ¡£æ‹¼æ¥ï¼Œä¸ºpackingæä¾›ææ–™ 
  enable_packing: false   # å…ˆç¦ç”¨packingç¡®ä¿åŸºç¡€åŠŸèƒ½æ­£å¸¸
  document_separator: "\n\n"
  add_special_tokens: true
  needle_in_haystack_prob: 0.05
  structured_task_prob: 0.1
  
  # æ•°æ®åŠ è½½
  num_workers: 0  # è®¾ä¸º0ä¾¿äºè°ƒè¯•
  pin_memory: true
  prefetch_factor: 2

# é•¿åº¦è¯¾ç¨‹å­¦ä¹ 
curriculum:
  enabled: false  # ç®€åŒ–é…ç½®ï¼Œç¦ç”¨è¯¾ç¨‹å­¦ä¹ 
  schedule: "standard"
  custom_schedule:
    - [4096, 1000]
    - [8192, 1000] 
    - [16384, 1000]
    - [32768, 5000]
  
device:
  # è®¾å¤‡é…ç½®
  use_gpu: true
  mixed_precision: "bf16"   # ä½¿ç”¨bf16æ··åˆç²¾åº¦è®­ç»ƒ
  compile_model: false      # å¦‚éœ€PyTorch 2.0ç¼–è¯‘ä¼˜åŒ–å¯è®¾ä¸ºtrue

logging:
  # æ—¥å¿—é…ç½®
  log_level: "INFO"         # DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_dir: "logs/"
  log_file: null            # å¦‚æœè®¾ç½®ï¼Œä¼šè¾“å‡ºåˆ°æ–‡ä»¶
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_interval: 10          # è®­ç»ƒæ—¥å¿—è®°å½•é—´éš”ï¼ˆstepsï¼‰
  save_interval: 100        # æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆstepsï¼‰
  output_dir: "checkpoints/" # æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
  
  # Wandbé…ç½® - å·²ç¦ç”¨
  wandb:
    enabled: false           # å…³é—­wandbè·Ÿè¸ª
    project: "fusion-tree-optimized"
    name: "optimized-pretrain"
    tags: ["hybrid", "mamba", "attention", "optimized", "srte-shared"]

# é—¨æ§ç›‘æ§é…ç½® - ä¸´æ—¶ç¦ç”¨é¿å…å¯å˜é•¿åº¦all_gatheré—®é¢˜
gate_monitor:
  enabled: false            # ç¦ç”¨ä»¥æ’é™¤å¯å˜é•¿åº¦all_gatherå¯¼è‡´çš„NCCLæ­»é”
  collect_detailed: false
  log_interval: 100
  save_interval: 1000
  save_path: "checkpoints/gate_stats.json"
  
  # è£å‰ªé˜ˆå€¼
  mamba_threshold_high: 0.8
  attention_threshold_low: 0.2
  min_steps_for_pruning: 500

# ç³»ç»Ÿé…ç½®
system:
  # åˆ†å¸ƒå¼è®­ç»ƒ
  distributed: true
  backend: "nccl"
  find_unused_parameters: false
  
  # å†…å­˜ä¼˜åŒ– - ä¿®å¤NCCLè¶…æ—¶é—®é¢˜
  use_deepspeed: true       # å¯ç”¨DeepSpeedï¼Œé™çº§åˆ°ZeRO-2
  zero_stage: 2             # ä»ZeRO-3é™çº§åˆ°ZeRO-2è§£å†³AllGatheræ­»é”
  offload_optimizer: false  # ğŸ”§ å…³é—­offloadè§£å†³backwardå¡é¡¿é—®é¢˜
  offload_params: false     # ZeRO-2ä¸æ”¯æŒå‚æ•°offload
  
  # ç¼–è¯‘ä¼˜åŒ–
  compile_model: false      # PyTorch 2.0 compile

# æ›¿ä»£é…ç½®é€‰é¡¹è¯´æ˜ï¼š
# 
# 1. è¶…ä½å‚æ•°ç‰ˆæœ¬ (~7Bå‚æ•°)ï¼š
#    hidden_size: 2048
#    num_layers: 24
#    srte_factorized_rank: 64
#
# 2. SinCosç‰ˆæœ¬ (SRTEæ¥è¿‘0å‚æ•°)ï¼š
#    srte_encoding: "sincos"
#    srte_share_across_layers: true
#    srte_factorized_rank: 0
#
# 3. é«˜æ€§èƒ½ç‰ˆæœ¬ (ä¿æŒhidden_size=4096)ï¼š
#    max_position_embeddings: 65536
#    srte_factorized_rank: 256 