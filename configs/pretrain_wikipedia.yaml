# HybridLanguageModel Wikipedia训练配置
# 使用真实Wikipedia数据进行预训练

training:
  max_steps: 5000        # 增加训练步数以充分利用真实数据
  warmup_steps: 200      # 4% 热身，更稳定
  learning_rate: 0.0001  # 1e-4
  batch_size: 4          # ↑ 每卡 micro-batch 提高至4
  gradient_accumulation_steps: 2   # ↓ 累积梯度步数降至2，保持总batch tokens不变
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 0.000001  # 1e-6 更稳定的数值设置
  
  # 学习率调度
  lr_scheduler: "cosine"
  min_lr_ratio: 0.1
  
  # 梯度相关
  max_grad_norm: 1.0
  gradient_checkpointing: true
  
  # 混合精度
  fp16: false
  bf16: true
  
  # 正则化系数 - 修复detach后的建议值
  load_balance_coeff: 0.05
  entropy_reg_coeff: 0.0003  # 3e-4 更强的熵正则
  distill_coeff: 0.0  # 设为 > 0 启用蒸馏
  distill_temperature: 4.0
  
  # 数据相关
  max_seq_length: 1024  # 🚀 优化: 恢复1024序列长度
  
  # 保存和评估 - 🚀 优化：降低保存频率防止NCCL超时
  save_steps: 1000      # ↑ 从500提高到1000，减少I/O阻塞
  eval_steps: 1000      # ↑ 对应提高评估频率
  logging_steps: 50     # 保持频繁的日志输出
  save_total_limit: 3   # ↑ 增加到3个，避免过于频繁的清理

model:
  # 🚀 优化后的中型模型配置 - 平衡性能与显存
  vocab_size: 50257          # 🔧 GPT2分词器的实际词汇大小
  hidden_size: 1536          # 🔧 优化: 768→1536 充分利用显存
  num_layers: 18             # 🔧 优化: 12→18 增加模型深度
  num_heads: 12              # 🔧 保持12个头适配1536维度
  window_size: 256           # 🔧 内存优化: 1024→256 减少局部注意力内存使用
  global_heads: 2
  gate_rank: 96              # 🔧 优化: 64→96 适度增加gate复杂度            
  max_position_embeddings: 8192   # 🔧 内存优化: 临时降低以减少RoPE缓存  
  
  # 关键优化：SRTE配置
  srte_encoding: "learnable"        
  srte_share_across_layers: true    
  srte_factorized_rank: 128         
  
  # 注意力配置
  attention_type: "local_global"    
  
  # 其他优化
  tie_word_embeddings: true         
  use_alignment: true
  
  # 正则化
  layer_norm_eps: 0.00001  # 1e-5
  dropout: 0.1
  drop_branch_prob: 0.1  # ✅ 已修复分布式随机性问题，可安全使用
  
  # 损失函数权重
  load_balance_coeff: 0.1
  entropy_reg_coeff: 0.0001  # 1e-4
  
  # 推理配置
  use_cache: true
  pad_token_id: 50256      # GPT2的PAD token
  bos_token_id: 50256      # GPT2使用同一个token作为BOS
  eos_token_id: 50256      # GPT2使用同一个token作为EOS

data:
  # 💾 Lazy JSONL加载模式 - 适合本地大文件
  data_mode: lazy           # lazy|static|hf_streaming
  max_samples_per_file: 2000  # 🧪 测试：每个文件只用2000样本
  
  train_data_paths:
    - "data/wikipedia/wiki_en/wiki_en_*.jsonl"
    # - "data/wikipedia/wiki_en/wiki_en_00000.jsonl"
    # - "data/wikipedia/wiki_en/wiki_en_00001.jsonl"
    # - "data/wikipedia/wiki_en/wiki_en_00002.jsonl"
  
  eval_data_paths:
    longbench:
      - "data/wikipedia/wiki_en/wiki_en_00000.jsonl"  # 用少量数据做验证
  
  # 预处理配置 - 针对真实数据优化
  tokenizer_path: "gpt2"     # 使用GPT2分词器，适合英文Wikipedia
  max_length: 1024           # 🚀 优化: 匹配training.max_seq_length
  min_length: 50             # 过滤过短文本
  add_special_tokens: true
  
  # 数据加载 - Lazy模式优化配置
  num_workers: 0          # 🔧 Lazy loading时推荐用0避免多进程文件访问冲突
  pin_memory: true
  prefetch_factor: 2

# 长度课程学习 - 暂时禁用，先在固定长度上稳定训练
curriculum:
  enabled: false
  schedule: "standard"
  custom_schedule:
    - [1024, 500]     # 先在1024长度训练500步
    - [2048, 300]     # 再增加到2048训练300步
    - [4096, 200]     # 最后增加到4096训练200步
  
device:
  # 设备配置
  use_gpu: true
  mixed_precision: "bf16"   
  compile_model: false      

logging:
  # 日志配置
  log_level: "INFO"         
  log_dir: "logs/"
  log_file: null            
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_interval: 10          # 训练日志记录间隔（steps）
  save_interval: 200        # 模型保存间隔（steps）
  output_dir: "/tmp/checkpoints/wikipedia/" # 🚀 优化：使用本地SSD避免网络I/O超时
  
  # Wandb配置 - 可选开启用于监控
  wandb:
    enabled: false           # 可设为true开启wandb跟踪
    project: "fusion-tree-wikipedia"
    name: "wikipedia-pretrain"
    tags: ["hybrid", "mamba", "attention", "wikipedia", "real-data"]

# 门控监控配置 - 可选开启
gate_monitor:
  enabled: false            # 可设为true监控门控行为
  collect_detailed: false
  log_interval: 100
  save_interval: 1000
  save_path: "/tmp/checkpoints/wikipedia/gate_stats.json"
  
  # 裁剪阈值
  mamba_threshold_high: 0.8
  attention_threshold_low: 0.2
  min_steps_for_pruning: 500

# 系统配置
system:
  # 分布式训练
  distributed: true
  backend: "nccl"
  find_unused_parameters: false
  
  # 内存优化 - 使用ZeRO-2获得更好吞吐
  use_deepspeed: true       
  zero_stage: 2              # ↓ 从ZeRO-3降级到ZeRO-2，更快的通信
  offload_optimizer: false  # 🔧 关闭offload避免I/O瓶颈和NCCL超时 
  offload_params: false     # ZeRO-2不支持参数offload     
  
  # 编译优化
  compile_model: false

# 数据集信息和使用说明
# 
# Wikipedia数据集特点:
# - 高质量百科全书文本
# - 平均文章长度适中(500-2000词)
# - 涵盖广泛主题
# - 适合作为预训练基础数据
#
# 使用方法:
# 1. 运行下载脚本: python scripts/download_datasets.py --dataset wikipedia --lang en
# 2. 检查数据路径: ls data/wikipedia/wiki_en/
# 3. 启动训练: bash scripts/run_pretrain.sh configs/pretrain_wikipedia.yaml [num_gpus]
#
# 预期效果:
# - 在真实数据上验证模型架构
# - 观察损失收敛情况
# - 为后续加入更多数据源做准备 