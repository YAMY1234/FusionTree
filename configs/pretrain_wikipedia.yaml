# HybridLanguageModel Wikipediaè®­ç»ƒé…ç½®
# ä½¿ç”¨çœŸå®Wikipediaæ•°æ®è¿›è¡Œé¢„è®­ç»ƒ

training:
  max_steps: 5000        # å¢åŠ è®­ç»ƒæ­¥æ•°ä»¥å……åˆ†åˆ©ç”¨çœŸå®æ•°æ®
  warmup_steps: 200      # 4% çƒ­èº«ï¼Œæ›´ç¨³å®š
  learning_rate: 0.0001  # 1e-4
  batch_size: 4          # â†‘ æ¯å¡ micro-batch æé«˜è‡³4
  gradient_accumulation_steps: 2   # â†“ ç´¯ç§¯æ¢¯åº¦æ­¥æ•°é™è‡³2ï¼Œä¿æŒæ€»batch tokensä¸å˜
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 0.000001  # 1e-6 æ›´ç¨³å®šçš„æ•°å€¼è®¾ç½®
  
  # å­¦ä¹ ç‡è°ƒåº¦
  lr_scheduler: "cosine"
  min_lr_ratio: 0.1
  
  # æ¢¯åº¦ç›¸å…³
  max_grad_norm: 1.0
  gradient_checkpointing: true
  
  # æ··åˆç²¾åº¦
  fp16: false
  bf16: true
  
  # æ­£åˆ™åŒ–ç³»æ•° - ä¿®å¤detachåçš„å»ºè®®å€¼
  load_balance_coeff: 0.05
  entropy_reg_coeff: 0.0003  # 3e-4 æ›´å¼ºçš„ç†µæ­£åˆ™
  distill_coeff: 0.0  # è®¾ä¸º > 0 å¯ç”¨è’¸é¦
  distill_temperature: 4.0
  
  # æ•°æ®ç›¸å…³
  max_seq_length: 1024  # ğŸš€ ä¼˜åŒ–: æ¢å¤1024åºåˆ—é•¿åº¦
  
  # ä¿å­˜å’Œè¯„ä¼° - ğŸš€ ä¼˜åŒ–ï¼šé™ä½ä¿å­˜é¢‘ç‡é˜²æ­¢NCCLè¶…æ—¶
  save_steps: 1000      # â†‘ ä»500æé«˜åˆ°1000ï¼Œå‡å°‘I/Oé˜»å¡
  eval_steps: 1000      # â†‘ å¯¹åº”æé«˜è¯„ä¼°é¢‘ç‡
  logging_steps: 50     # ä¿æŒé¢‘ç¹çš„æ—¥å¿—è¾“å‡º
  save_total_limit: 3   # â†‘ å¢åŠ åˆ°3ä¸ªï¼Œé¿å…è¿‡äºé¢‘ç¹çš„æ¸…ç†

model:
  # ğŸš€ ä¼˜åŒ–åçš„ä¸­å‹æ¨¡å‹é…ç½® - å¹³è¡¡æ€§èƒ½ä¸æ˜¾å­˜
  vocab_size: 50257          # ğŸ”§ GPT2åˆ†è¯å™¨çš„å®é™…è¯æ±‡å¤§å°
  hidden_size: 1536          # ğŸ”§ ä¼˜åŒ–: 768â†’1536 å……åˆ†åˆ©ç”¨æ˜¾å­˜
  num_layers: 18             # ğŸ”§ ä¼˜åŒ–: 12â†’18 å¢åŠ æ¨¡å‹æ·±åº¦
  num_heads: 12              # ğŸ”§ ä¿æŒ12ä¸ªå¤´é€‚é…1536ç»´åº¦
  window_size: 256           # ğŸ”§ å†…å­˜ä¼˜åŒ–: 1024â†’256 å‡å°‘å±€éƒ¨æ³¨æ„åŠ›å†…å­˜ä½¿ç”¨
  global_heads: 2
  gate_rank: 96              # ğŸ”§ ä¼˜åŒ–: 64â†’96 é€‚åº¦å¢åŠ gateå¤æ‚åº¦            
  max_position_embeddings: 8192   # ğŸ”§ å†…å­˜ä¼˜åŒ–: ä¸´æ—¶é™ä½ä»¥å‡å°‘RoPEç¼“å­˜  
  
  # å…³é”®ä¼˜åŒ–ï¼šSRTEé…ç½®
  srte_encoding: "learnable"        
  srte_share_across_layers: true    
  srte_factorized_rank: 128         
  
  # æ³¨æ„åŠ›é…ç½®
  attention_type: "local_global"    
  
  # å…¶ä»–ä¼˜åŒ–
  tie_word_embeddings: true         
  use_alignment: true
  
  # æ­£åˆ™åŒ–
  layer_norm_eps: 0.00001  # 1e-5
  dropout: 0.1
  drop_branch_prob: 0.1  # âœ… å·²ä¿®å¤åˆ†å¸ƒå¼éšæœºæ€§é—®é¢˜ï¼Œå¯å®‰å…¨ä½¿ç”¨
  
  # æŸå¤±å‡½æ•°æƒé‡
  load_balance_coeff: 0.1
  entropy_reg_coeff: 0.0001  # 1e-4
  
  # æ¨ç†é…ç½®
  use_cache: true
  pad_token_id: 50256      # GPT2çš„PAD token
  bos_token_id: 50256      # GPT2ä½¿ç”¨åŒä¸€ä¸ªtokenä½œä¸ºBOS
  eos_token_id: 50256      # GPT2ä½¿ç”¨åŒä¸€ä¸ªtokenä½œä¸ºEOS

data:
  # ğŸ’¾ Lazy JSONLåŠ è½½æ¨¡å¼ - é€‚åˆæœ¬åœ°å¤§æ–‡ä»¶
  data_mode: lazy           # lazy|static|hf_streaming
  max_samples_per_file: 2000  # ğŸ§ª æµ‹è¯•ï¼šæ¯ä¸ªæ–‡ä»¶åªç”¨2000æ ·æœ¬
  
  train_data_paths:
    - "data/wikipedia/wiki_en/wiki_en_*.jsonl"
    # - "data/wikipedia/wiki_en/wiki_en_00000.jsonl"
    # - "data/wikipedia/wiki_en/wiki_en_00001.jsonl"
    # - "data/wikipedia/wiki_en/wiki_en_00002.jsonl"
  
  eval_data_paths:
    longbench:
      - "data/wikipedia/wiki_en/wiki_en_00000.jsonl"  # ç”¨å°‘é‡æ•°æ®åšéªŒè¯
  
  # é¢„å¤„ç†é…ç½® - é’ˆå¯¹çœŸå®æ•°æ®ä¼˜åŒ–
  tokenizer_path: "gpt2"     # ä½¿ç”¨GPT2åˆ†è¯å™¨ï¼Œé€‚åˆè‹±æ–‡Wikipedia
  max_length: 1024           # ğŸš€ ä¼˜åŒ–: åŒ¹é…training.max_seq_length
  min_length: 50             # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬
  add_special_tokens: true
  
  # æ•°æ®åŠ è½½ - Lazyæ¨¡å¼ä¼˜åŒ–é…ç½®
  num_workers: 0          # ğŸ”§ Lazy loadingæ—¶æ¨èç”¨0é¿å…å¤šè¿›ç¨‹æ–‡ä»¶è®¿é—®å†²çª
  pin_memory: true
  prefetch_factor: 2

# é•¿åº¦è¯¾ç¨‹å­¦ä¹  - æš‚æ—¶ç¦ç”¨ï¼Œå…ˆåœ¨å›ºå®šé•¿åº¦ä¸Šç¨³å®šè®­ç»ƒ
curriculum:
  enabled: false
  schedule: "standard"
  custom_schedule:
    - [1024, 500]     # å…ˆåœ¨1024é•¿åº¦è®­ç»ƒ500æ­¥
    - [2048, 300]     # å†å¢åŠ åˆ°2048è®­ç»ƒ300æ­¥
    - [4096, 200]     # æœ€åå¢åŠ åˆ°4096è®­ç»ƒ200æ­¥
  
device:
  # è®¾å¤‡é…ç½®
  use_gpu: true
  mixed_precision: "bf16"   
  compile_model: false      

logging:
  # æ—¥å¿—é…ç½®
  log_level: "INFO"         
  log_dir: "logs/"
  log_file: null            
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_interval: 10          # è®­ç»ƒæ—¥å¿—è®°å½•é—´éš”ï¼ˆstepsï¼‰
  save_interval: 200        # æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆstepsï¼‰
  output_dir: "/tmp/checkpoints/wikipedia/" # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨æœ¬åœ°SSDé¿å…ç½‘ç»œI/Oè¶…æ—¶
  
  # Wandbé…ç½® - å¯é€‰å¼€å¯ç”¨äºç›‘æ§
  wandb:
    enabled: false           # å¯è®¾ä¸ºtrueå¼€å¯wandbè·Ÿè¸ª
    project: "fusion-tree-wikipedia"
    name: "wikipedia-pretrain"
    tags: ["hybrid", "mamba", "attention", "wikipedia", "real-data"]

# é—¨æ§ç›‘æ§é…ç½® - å¯é€‰å¼€å¯
gate_monitor:
  enabled: false            # å¯è®¾ä¸ºtrueç›‘æ§é—¨æ§è¡Œä¸º
  collect_detailed: false
  log_interval: 100
  save_interval: 1000
  save_path: "/tmp/checkpoints/wikipedia/gate_stats.json"
  
  # è£å‰ªé˜ˆå€¼
  mamba_threshold_high: 0.8
  attention_threshold_low: 0.2
  min_steps_for_pruning: 500

# ç³»ç»Ÿé…ç½®
system:
  # åˆ†å¸ƒå¼è®­ç»ƒ
  distributed: true
  backend: "nccl"
  find_unused_parameters: false
  
  # å†…å­˜ä¼˜åŒ– - ä½¿ç”¨ZeRO-2è·å¾—æ›´å¥½åå
  use_deepspeed: true       
  zero_stage: 2              # â†“ ä»ZeRO-3é™çº§åˆ°ZeRO-2ï¼Œæ›´å¿«çš„é€šä¿¡
  offload_optimizer: false  # ğŸ”§ å…³é—­offloadé¿å…I/Oç“¶é¢ˆå’ŒNCCLè¶…æ—¶ 
  offload_params: false     # ZeRO-2ä¸æ”¯æŒå‚æ•°offload     
  
  # ç¼–è¯‘ä¼˜åŒ–
  compile_model: false

# æ•°æ®é›†ä¿¡æ¯å’Œä½¿ç”¨è¯´æ˜
# 
# Wikipediaæ•°æ®é›†ç‰¹ç‚¹:
# - é«˜è´¨é‡ç™¾ç§‘å…¨ä¹¦æ–‡æœ¬
# - å¹³å‡æ–‡ç« é•¿åº¦é€‚ä¸­(500-2000è¯)
# - æ¶µç›–å¹¿æ³›ä¸»é¢˜
# - é€‚åˆä½œä¸ºé¢„è®­ç»ƒåŸºç¡€æ•°æ®
#
# ä½¿ç”¨æ–¹æ³•:
# 1. è¿è¡Œä¸‹è½½è„šæœ¬: python scripts/download_datasets.py --dataset wikipedia --lang en
# 2. æ£€æŸ¥æ•°æ®è·¯å¾„: ls data/wikipedia/wiki_en/
# 3. å¯åŠ¨è®­ç»ƒ: bash scripts/run_pretrain.sh configs/pretrain_wikipedia.yaml [num_gpus]
#
# é¢„æœŸæ•ˆæœ:
# - åœ¨çœŸå®æ•°æ®ä¸ŠéªŒè¯æ¨¡å‹æ¶æ„
# - è§‚å¯ŸæŸå¤±æ”¶æ•›æƒ…å†µ
# - ä¸ºåç»­åŠ å…¥æ›´å¤šæ•°æ®æºåšå‡†å¤‡ 