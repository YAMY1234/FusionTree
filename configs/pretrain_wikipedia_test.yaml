# HybridLanguageModel Wikipedia测试配置
# 减少显存使用，测试保存功能
# 基于 pretrain_wikipedia.yaml 调整

training:
  max_steps: 100           # 🧪 减少到100步用于测试
  warmup_steps: 10         # 对应减少热身步数
  learning_rate: 0.0001
  batch_size: 2            # 🔧 从4降到2，减少显存使用
  gradient_accumulation_steps: 4   # 🔧 从2增到4，保持总batch size不变
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 0.000001
  
  # 学习率调度
  lr_scheduler: "cosine"
  min_lr_ratio: 0.1
  
  # 梯度相关
  max_grad_norm: 1.0
  gradient_checkpointing: true
  
  # 混合精度
  fp16: false
  bf16: true
  
  # 正则化系数
  load_balance_coeff: 0.05
  entropy_reg_coeff: 0.0003
  distill_coeff: 0.0
  distill_temperature: 4.0
  
  # 数据相关
  max_seq_length: 512      # 🔧 从1024降到512，减少显存使用
  
  # 保存和评估 - 🧪 更频繁保存测试checkpoint功能
  save_steps: 20           # 🔧 每20步保存一次，测试保存功能
  eval_steps: 20           # 对应调整评估频率
  logging_steps: 5         # 更频繁的日志输出
  save_total_limit: 5      # 保留更多checkpoint用于测试

model:
  # 🔧 减小模型尺寸以适应显存限制
  vocab_size: 50257
  hidden_size: 1024        # 🔧 从1536降到1024
  num_layers: 12           # 🔧 从18降到12
  num_heads: 8             # 🔧 从12降到8，适配1024维度
  window_size: 128         # 🔧 从256降到128
  global_heads: 2
  gate_rank: 64            # 🔧 从96降到64
  max_position_embeddings: 4096   # 🔧 从8192降到4096
  
  # SRTE配置
  srte_encoding: "learnable"
  srte_share_across_layers: true
  srte_factorized_rank: 64         # 🔧 从128降到64，减少参数量
  
  # 注意力配置
  attention_type: "local_global"
  
  # 其他优化
  tie_word_embeddings: true
  use_alignment: true
  
  # 正则化
  layer_norm_eps: 0.00001
  dropout: 0.1
  drop_branch_prob: 0.1
  
  # 损失函数权重
  load_balance_coeff: 0.1
  entropy_reg_coeff: 0.0001
  
  # 推理配置
  use_cache: true
  pad_token_id: 50256
  bos_token_id: 50256
  eos_token_id: 50256

data:
  # 数据配置
  data_mode: lazy
  max_samples_per_file: 500   # 🔧 减少样本数量加快测试
  
  train_data_paths:
    - "data/wikipedia/wiki_en/wiki_en_00000.jsonl"  # 🔧 只用一个文件测试
  
  eval_data_paths:
    longbench:
      - "data/wikipedia/wiki_en/wiki_en_00000.jsonl"
  
  # 预处理配置
  tokenizer_path: "gpt2"
  max_length: 512            # 🔧 匹配training.max_seq_length
  min_length: 50
  add_special_tokens: true
  
  # 数据加载
  num_workers: 0
  pin_memory: true
  prefetch_factor: 2

# 长度课程学习 - 禁用
curriculum:
  enabled: false
  
device:
  # 设备配置
  use_gpu: true
  mixed_precision: "bf16"
  compile_model: false

logging:
  # 日志配置
  log_level: "INFO"
  log_dir: "logs/"
  log_file: null
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_interval: 5           # 更频繁的日志
  save_interval: 20         # 匹配save_steps
  output_dir: "/tmp/checkpoints/wikipedia_test/"  # 🔧 专门的测试目录
  
  # Wandb配置
  wandb:
    enabled: false
    project: "fusion-tree-test"
    name: "wikipedia-test"
    tags: ["test", "memory-optimized", "checkpoint-test"]

# 门控监控配置
gate_monitor:
  enabled: false

# 系统配置
system:
  # 分布式训练
  distributed: true
  backend: "nccl"
  find_unused_parameters: false
  
  # 内存优化 - 🔧 启用更激进的内存优化
  use_deepspeed: true
  zero_stage: 3              # 🔧 使用ZeRO-3获得最大内存节省
  offload_optimizer: true    # 🔧 启用优化器offload进一步节省显存
  offload_params: true       # 🔧 启用参数offload
  
  # 编译优化
  compile_model: false

# 测试说明:
# 这个配置用于测试：
# 1. DeepSpeed checkpoint保存功能是否正常工作
# 2. 较小的模型是否能在当前GPU上运行
# 3. 更频繁的保存是否会导致问题
#
# 与原配置的主要差异：
# - 模型尺寸减小约50%
# - 序列长度减半
# - batch_size减半但保持总batch tokens相同
# - 启用ZeRO-3和offload以最大化内存节省
# - 每20步保存一次checkpoint
# - 总训练步数减少到100步 