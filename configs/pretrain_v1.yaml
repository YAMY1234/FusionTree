# FusionTree 预训练配置 V1
# 支持长度课程学习的混合架构模型配置

# 模型配置
model:
  vocab_size: 50432
  hidden_size: 4096
  num_layers: 32
  num_heads: 32
  window_size: 1024
  global_heads: 2
  gate_rank: 256
  max_position_embeddings: 65536
  layer_norm_eps: 1e-5
  dropout: 0.1
  drop_branch_prob: 0.1
  attention_type: "local_global"  # "local_global" or "pyramidal"
  srte_encoding: "learnable"  # "learnable" or "sincos"
  use_alignment: true
  tie_word_embeddings: false

# 训练配置
training:
  # 基础设置
  batch_size: 4
  gradient_accumulation_steps: 8
  max_steps: 25000
  warmup_steps: 1000
  
  # 优化器
  optimizer: "adamw"
  learning_rate: 3e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  
  # 学习率调度
  lr_scheduler: "cosine"
  min_lr_ratio: 0.1
  
  # 梯度相关
  max_grad_norm: 1.0
  gradient_checkpointing: true
  
  # 混合精度
  fp16: false
  bf16: true
  
  # 正则化系数
  load_balance_coeff: 0.1
  entropy_reg_coeff: 1e-4
  distill_coeff: 0.0  # 设为 > 0 启用蒸馏
  distill_temperature: 4.0

# 长度课程学习
curriculum:
  enabled: true
  schedule: "standard"  # "standard", "aggressive", "conservative" 
  # 自定义课程（如果不用预设）
  custom_schedule:
    - [4096, 5000]    # [max_length, steps]
    - [8192, 5000]
    - [16384, 5000]
    - [32768, 10000]

# 数据配置
data:
  train_data_paths:
    - "data/train/pile_subset.jsonl"
    - "data/train/books.jsonl" 
    - "data/train/code.jsonl"
  
  eval_data_paths:
    longbench:
      - "data/eval/longbench/qasper.jsonl"
      - "data/eval/longbench/multifieldqa.jsonl"
    needle:
      - "data/eval/needle_in_haystack.jsonl"
    code:
      - "data/eval/code/repobench.jsonl"
  
  # 数据处理
  concat_docs: true
  enable_packing: true
  document_separator: "\n\n"
  add_special_tokens: true
  needle_in_haystack_prob: 0.05
  structured_task_prob: 0.1
  
  # 数据加载
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2

# 门控监控
gate_monitor:
  enabled: true
  collect_detailed: true
  log_interval: 100
  save_interval: 1000
  save_path: "checkpoints/gate_stats.json"
  
  # 裁剪阈值
  mamba_threshold_high: 0.8
  attention_threshold_low: 0.2
  min_steps_for_pruning: 500

# 日志和检查点
logging:
  log_level: "INFO"
  log_interval: 10
  eval_interval: 1000
  save_interval: 2000
  
  # Wandb配置
  wandb:
    enabled: true
    project: "fusion-tree"
    name: "v1-pretrain"
    tags: ["hybrid", "mamba", "attention", "long-context"]
  
  # 输出目录
  output_dir: "checkpoints/v1"
  save_total_limit: 5
  
# 评估配置
evaluation:
  eval_batch_size: 2
  max_eval_samples: 500
  
  # 评估任务
  tasks:
    - "perplexity"
    - "longbench_qa"
    - "needle_in_haystack"
    - "code_completion"
  
  # 生成参数
  generation:
    max_new_tokens: 256
    temperature: 1.0
    top_p: 0.9
    do_sample: true

# 系统配置
system:
  # 分布式训练
  distributed: true
  backend: "nccl"
  find_unused_parameters: false
  
  # 内存优化
  zero_stage: 2
  offload_optimizer: false
  offload_params: false
  
  # 编译优化
  compile_model: false  # PyTorch 2.0 compile
  
  # 调试
  detect_anomaly: false
  profile: false

# 推理配置
inference:
  use_cache: true
  batch_size: 1
  max_length: 32768
  
  # 导出设置
  export_pruned: true
  prune_plan_path: "checkpoints/prune_plan.json"
  
# 硬件配置
hardware:
  num_gpus: 8
  gpu_memory_limit: 80  # GB per GPU
  
  # 性能监控
  monitor_gpu_usage: true
  monitor_memory: true
  
# 实验配置
experiment:
  seed: 42
  deterministic: false
  benchmark: true
  
  # 消融实验开关
  ablation:
    disable_mamba: false
    disable_attention: false
    disable_gate: false
    disable_alignment: false
    disable_srte: false 