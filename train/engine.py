"""
è®­ç»ƒå¼•æ“ï¼šæ··åˆæ¨¡å‹çš„è®­ç»ƒå¾ªç¯å’Œä¼˜åŒ–
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
import argparse
import logging
from typing import Dict, Any, Optional
import wandb
from tqdm import tqdm
import json  # ğŸ”§ æ·»åŠ jsonå¯¼å…¥

# å°è¯•å¯¼å…¥DeepSpeed
try:
    import deepspeed
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False
    print("DeepSpeed not available, falling back to FSDP")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.hybrid_model import HybridLanguageModel, HybridLanguageModelConfig
from train.data import create_data_loader, CURRICULUM_SCHEDULES
from train.losses import HybridModelLoss
from train.monitor_gate import GateMonitor

logger = logging.getLogger(__name__)


class TrainingEngine:
    """æ··åˆæ¨¡å‹è®­ç»ƒå¼•æ“"""
    
    def __init__(self, config_path: str, distributed: bool = False):
        self.config_path = config_path
        self.distributed = distributed
        self.rank = 0
        self.world_size = 1
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼
        if distributed:
            self._init_distributed()
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device(f'cuda:{self.rank}')
        torch.cuda.set_device(self.device)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_model()
        self._init_tokenizer()
        self._init_data()
        self._init_optimizer()
        self._init_deepspeed()  # åœ¨ä¼˜åŒ–å™¨ä¹‹ååˆå§‹åŒ–DeepSpeed
        self._init_loss()
        self._init_monitor()
        
        # åˆå§‹åŒ–wandb
        if self.config['logging']['wandb']['enabled'] and self.rank == 0:
            wandb.init(
                project=self.config['logging']['wandb']['project'],
                name=self.config['logging']['wandb']['name'],
                tags=self.config['logging']['wandb']['tags'],
                config=self.config
            )
    
    def _init_distributed(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
        dist.init_process_group(backend='nccl')
        self.rank = dist.get_rank()
        self.world_size = dist.get_world_size()
        
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_level = getattr(logging, self.config['logging']['log_level'])
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        model_config = HybridLanguageModelConfig(**self.config['model'])
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨DeepSpeed
        if DEEPSPEED_AVAILABLE and self.config['system'].get('use_deepspeed', True):
            # ä½¿ç”¨DeepSpeedæ—¶ä¸ç›´æ¥ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
            self.model = HybridLanguageModel(model_config)
            self.use_deepspeed = True
            logger.info("Using DeepSpeed for memory optimization")
        else:
            # é™çº§åˆ°FSDPæˆ–æ ‡å‡†DDP
            self.model = HybridLanguageModel(model_config)
            
            # åˆ†æ‰¹ç§»åŠ¨æ¨¡å‹å‚æ•°åˆ°è®¾å¤‡ä»¥é¿å…OOM
            logger.info("Moving model to device in chunks to avoid OOM...")
            self._move_model_to_device_safely()
            
            # åˆ†å¸ƒå¼åŒ…è£…
            if self.distributed:
                self.model = DDP(
                    self.model, 
                    device_ids=[self.rank],
                    find_unused_parameters=self.config['system']['find_unused_parameters']
                )
            
            self.use_deepspeed = False
        
        # ç¼–è¯‘æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config['system']['compile_model']:
            self.model = torch.compile(self.model)
            
        logger.info(f"Model initialized with {sum(p.numel() for p in self.model.parameters())/1e6:.1f}M parameters")
    
    def _move_model_to_device_safely(self):
        """åˆ†æ‰¹ç§»åŠ¨æ¨¡å‹å‚æ•°åˆ°è®¾å¤‡ä»¥é¿å…OOM"""
        # æ¸…ç©ºç¼“å­˜
        torch.cuda.empty_cache()
        
        # é€å±‚ç§»åŠ¨æ¨¡å‹
        for name, module in self.model.named_children():
            logger.info(f"Moving {name} to device...")
            module.to(self.device)
            torch.cuda.empty_cache()  # æ¯æ¬¡ç§»åŠ¨åæ¸…ç©ºç¼“å­˜
    
    def _init_tokenizer(self):
        """åˆå§‹åŒ–åˆ†è¯å™¨"""
        # è¿™é‡Œä½¿ç”¨å ä½ç¬¦ï¼Œå®é™…åº”è¯¥åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨
        try:
            self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # è®¾ç½®model_max_lengthä»¥é¿å…é•¿åº¦è­¦å‘Šï¼Œä½¿ç”¨æˆ‘ä»¬æ¨¡å‹çš„window_size
            model_config = self.config.get('model', {})
            model_max_length = model_config.get('window_size', 1024)
            self.tokenizer.model_max_length = model_max_length
            logger.info(f"Set tokenizer max_length to {model_max_length}")
            
        except:
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½ç¬¦
            model_config = self.config.get('model', {})
            window_size = model_config.get('window_size', 1024)
            
            class DummyTokenizer:
                def __init__(self):
                    self.pad_token_id = 0
                    self.bos_token_id = 1
                    self.eos_token_id = 2
                    self.model_max_length = window_size
                    
                def encode(self, text, add_special_tokens=False, max_length=None, truncation=False):
                    # ç®€å•çš„å­—ç¬¦çº§ç¼–ç 
                    tokens = [ord(c) % 1000 for c in text[:100]]
                    if max_length and truncation and len(tokens) > max_length:
                        tokens = tokens[:max_length]
                    return tokens
                
                def __call__(self, text, max_length=None, truncation=False, **kwargs):
                    tokens = self.encode(text, max_length=max_length, truncation=truncation)
                    input_ids = torch.tensor([tokens])
                    return {'input_ids': input_ids, 'attention_mask': torch.ones_like(input_ids)}
            
            self.tokenizer = DummyTokenizer()
            logger.warning("Using dummy tokenizer for testing")
    
    def _init_data(self):
        """åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨"""
        data_config = self.config['data']
        training_config = self.config['training']
        model_config = self.config['model']
        
        # è·å–è¯¾ç¨‹å­¦ä¹ è®¡åˆ’
        curriculum_schedule = None
        if self.config['curriculum']['enabled']:
            schedule_name = self.config['curriculum']['schedule']
            if schedule_name in CURRICULUM_SCHEDULES:
                curriculum_schedule = CURRICULUM_SCHEDULES[schedule_name]
            else:
                curriculum_schedule = self.config['curriculum']['custom_schedule']
        
        # ä½¿ç”¨æ¨¡å‹çš„window_sizeä½œä¸ºæœ€å¤§åºåˆ—é•¿åº¦ï¼Œç¡®ä¿ä¸æ¨¡å‹é…ç½®ä¸€è‡´
        max_seq_length = model_config.get('window_size', 1024)
        logger.info(f"Setting data max_length to {max_seq_length} (from model.window_size)")
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
        logger.info(f"[DEBUG] Creating train dataloader with:")
        logger.info(f"  - data_paths: {data_config['train_data_paths']}")
        logger.info(f"  - batch_size: {training_config['batch_size']}")
        logger.info(f"  - max_length: {max_seq_length}")
        logger.info(f"  - distributed: {self.distributed}")
        
        # ğŸš€ å¯æ‰©å±•çš„æ•°æ®åŠ è½½ç³»ç»Ÿ
        data_mode = data_config.get('data_mode', 'static')  # static|lazy|hf_streaming
        
        logger.info(f"Data loading mode: {data_mode}")
        
        if data_mode == 'hf_streaming':
            # ğŸ”¥ HuggingFaceæµå¼åŠ è½½ - å·¥ä¸šç•Œæœ€ä½³å®è·µ
            from train.streaming_data import create_streaming_data_loader, get_preset_sources
            
            # è·å–æ•°æ®æºé…ç½®
            if 'hf_streaming_sources' in data_config:
                sources = data_config['hf_streaming_sources']
            elif 'hf_preset' in data_config:
                sources = get_preset_sources(data_config['hf_preset'])
                logger.info(f"Using preset '{data_config['hf_preset']}' with {len(sources)} sources")
            else:
                raise ValueError("hf_streaming mode requires 'hf_streaming_sources' or 'hf_preset' in config")
            
            logger.info("Using HFStreamingIterableDataset for maximum efficiency")
            self.train_loader = create_streaming_data_loader(
                sources=sources,
                tokenizer=self.tokenizer,
                batch_size=training_config['batch_size'],
                max_length=max_seq_length,
                num_workers=data_config.get('num_workers', 0),  # æµå¼æ¨è0
                seed=data_config.get('seed', 42),
                pin_memory=data_config.get('pin_memory', True),
                concat_docs=data_config.get('concat_docs', True),
                min_length=data_config.get('min_length', 10),
                add_special_tokens=data_config.get('add_special_tokens', True),
                document_separator=data_config.get('document_separator', '\n\n'),
                shuffle_buffer_size=data_config.get('shuffle_buffer_size', 10000),
                trust_remote_code=data_config.get('trust_remote_code', False)
            )
            
        elif data_mode == 'lazy':
            # ğŸ’¾ Lazy JSONLåŠ è½½ - é€‚åˆæœ¬åœ°å¤§æ–‡ä»¶
            from train.lazy_data import create_lazy_data_loader
            logger.info("Using LazyJSONLDataset for memory-efficient loading")
            
            self.train_loader = create_lazy_data_loader(
                data_paths=data_config['train_data_paths'],
                tokenizer=self.tokenizer,
                batch_size=training_config['batch_size'],
                max_length=max_seq_length,
                num_workers=data_config.get('num_workers', 0),
                shuffle=True,
                distributed=self.distributed,
                max_samples_per_file=data_config.get('max_samples_per_file', None),
                min_length=data_config.get('min_length', 10),
                add_special_tokens=data_config.get('add_special_tokens', True)
            )
            
        elif data_mode == 'static':
            # ğŸ“ ä¼ ç»Ÿé™æ€åŠ è½½ - é€‚åˆå°æ•°æ®é›†å’Œè°ƒè¯•
            logger.info("Using traditional LongContextDataset (static loading)")
            self.train_loader = create_data_loader(
                data_paths=data_config['train_data_paths'],
                tokenizer=self.tokenizer,
                batch_size=training_config['batch_size'],
                max_length=max_seq_length,  # ä½¿ç”¨æ¨¡å‹window_size
                num_workers=data_config['num_workers'],
                shuffle=True,
                distributed=self.distributed,  # ä¼ é€’åˆ†å¸ƒå¼æ ‡å¿—
                curriculum_schedule=curriculum_schedule,
                **{k: v for k, v in data_config.items() 
                   if k not in ['train_data_paths', 'eval_data_paths', 'num_workers', 'max_length', 'tokenizer_path', 'data_mode']}
            )
            
        else:
            raise ValueError(f"Unknown data_mode: {data_mode}. Supported: static|lazy|hf_streaming")
        
        # å…³é”®DEBUGä¿¡æ¯
        if hasattr(self.train_loader, 'dataset'):
            dataset_len = len(self.train_loader.dataset)
            logger.info(f"[DEBUG] len(train_dataset) = {dataset_len}")
        else:
            logger.info(f"[DEBUG] train_loader has no dataset attribute")
            
        dataloader_len = len(self.train_loader)
        logger.info(f"[DEBUG] batch_size = {training_config['batch_size']}")
        logger.info(f"[DEBUG] len(train_dataloader) = {dataloader_len}")
        
        if dataloader_len == 0:
            raise RuntimeError(
                f"Empty train_dataloader detected! Dataset length: {dataset_len if 'dataset_len' in locals() else 'unknown'}, "
                f"batch_size: {training_config['batch_size']}. è¯·æ£€æŸ¥æ•°æ®è·¯å¾„/æ ¼å¼/è¿‡æ»¤ä¸packingé…ç½®ã€‚"
            )
        
        logger.info(f"Training data loader created with {len(self.train_loader)} batches")
    
    def _init_optimizer(self):
        """åˆå§‹åŒ–ä¼˜åŒ–å™¨"""
        training_config = self.config['training']
        
        # åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆæ— è®ºæ˜¯å¦ä½¿ç”¨DeepSpeedéƒ½éœ€è¦å…ˆåˆ›å»ºï¼‰
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=training_config['learning_rate'],
            betas=(training_config['beta1'], training_config['beta2']),
            eps=training_config['eps'],
            weight_decay=training_config['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if training_config['lr_scheduler'] == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=training_config['max_steps'],
                eta_min=training_config['learning_rate'] * training_config['min_lr_ratio']
            )
        else:
            self.scheduler = None
            
        if hasattr(self, 'use_deepspeed') and self.use_deepspeed:
            logger.info("Optimizer will be initialized by DeepSpeed")
    
    def _init_deepspeed(self):
        """åˆå§‹åŒ–DeepSpeedï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        if not (hasattr(self, 'use_deepspeed') and self.use_deepspeed):
            return
        
        if not DEEPSPEED_AVAILABLE:
            logger.error("DeepSpeed not available but use_deepspeed=True")
            return
        
        training_config = self.config['training']
        
        # âœ… ä»é…ç½®è¯»å–ZeRO stageè€Œä¸æ˜¯ç¡¬ç¼–ç 
        system_config = self.config['system']
        zero_stage = int(system_config.get('zero_stage', 2))
        offload_optimizer = system_config.get('offload_optimizer', False)
        offload_params = system_config.get('offload_params', False)
        
        logger.info(f"DeepSpeed ZeRO stage: {zero_stage}, offload_optimizer: {offload_optimizer}, offload_params: {offload_params}")
        
        # æ„å»ºzero_optimizationé…ç½®
        zero_config = {
            "stage": zero_stage,
            "overlap_comm": True,
            "contiguous_gradients": True,
            "sub_group_size": 1e9,
            "reduce_bucket_size": "auto",
        }
        
        # æ ¹æ®stageæ·»åŠ ç›¸åº”é…ç½®
        if zero_stage == 3:
            zero_config.update({
                "stage3_prefetch_bucket_size": "auto",
                "stage3_param_persistence_threshold": "auto", 
                "stage3_max_live_parameters": 1e9,
                "stage3_max_reuse_distance": 1e9,
                "gather_16bit_weights_on_model_save": True
            })
            
        # é…ç½®offloadï¼ˆæ³¨æ„ZeRO-2ä¸æ”¯æŒå‚æ•°offloadï¼‰
        if offload_optimizer:
            zero_config["offload_optimizer"] = {"device": "cpu"}
        else:
            zero_config["offload_optimizer"] = {"device": "none"}
            
        if zero_stage == 3 and offload_params:
            zero_config["offload_param"] = {"device": "cpu"}
        else:
            zero_config["offload_param"] = {"device": "none"}
        
        # è®¾ç½®DeepSpeedé…ç½® - å®Œå…¨ç¦ç”¨è‡ªå®šä¹‰ä¼˜åŒ–å™¨é¿å…GCCç¼–è¯‘é—®é¢˜
        ds_config = {
            "train_batch_size": training_config['batch_size'] * training_config['gradient_accumulation_steps'] * self.world_size,
            "train_micro_batch_size_per_gpu": training_config['batch_size'],
            "gradient_accumulation_steps": training_config['gradient_accumulation_steps'],
            # ç§»é™¤optimizeré…ç½®ï¼Œå¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬ä¼ å…¥çš„PyTorchåŸç”Ÿä¼˜åŒ–å™¨
            # "optimizer": {...},  # æ³¨é‡Šæ‰é¿å…FusedAdamç¼–è¯‘
            
            # âœ… è®©DeepSpeedç®¡ç†å­¦ä¹ ç‡è°ƒåº¦ï¼Œé¿å…è°ƒç”¨é¡ºåºé—®é¢˜
            "scheduler": {
                "type": "WarmupCosineLR",
                "params": {
                    "total_num_steps": training_config['max_steps'],
                    "warmup_num_steps": training_config['warmup_steps']
                }
            },
            
            "gradient_clipping": training_config['max_grad_norm'],
            "fp16": {"enabled": training_config.get('fp16', False)},
            "bf16": {"enabled": training_config.get('bf16', True)},
            "zero_optimization": zero_config,
            # âœ… å…è®¸ä½¿ç”¨è‡ªå®šä¹‰PyTorchä¼˜åŒ–å™¨ä¸ZeRO-Offload
            "zero_force_ds_cpu_optimizer": False
        }
        
        # åˆå§‹åŒ–DeepSpeed - ä¼ å…¥æˆ‘ä»¬çš„PyTorchåŸç”Ÿä¼˜åŒ–å™¨ï¼Œè®©DSç®¡ç†scheduler
        logger.info("Initializing DeepSpeed engine...")
        self.model, self.optimizer, _, self.scheduler = deepspeed.initialize(
            model=self.model,
            optimizer=self.optimizer,  # ä½¿ç”¨æˆ‘ä»¬å·²åˆ›å»ºçš„PyTorch AdamW
            lr_scheduler=None,  # âœ… è®©DeepSpeedè‡ªå·±åˆ›å»ºå’Œç®¡ç†scheduler
            config=ds_config,
            dist_init_required=False  # å·²ç»åˆå§‹åŒ–äº†åˆ†å¸ƒå¼
        )
        
        logger.info("DeepSpeed engine initialized successfully")
    
    def _init_loss(self):
        """åˆå§‹åŒ–æŸå¤±å‡½æ•°"""
        training_config = self.config['training']
        
        self.loss_fn = HybridModelLoss(
            load_balance_coeff=training_config['load_balance_coeff'],
            entropy_reg_coeff=training_config['entropy_reg_coeff'],
            distill_coeff=training_config['distill_coeff'],
            distill_temperature=training_config['distill_temperature']
        )
    
    def _init_monitor(self):
        """åˆå§‹åŒ–é—¨æ§ç›‘æ§å™¨"""
        gate_config = self.config['gate_monitor']
        if gate_config['enabled']:
            self.gate_monitor = GateMonitor(
                num_layers=self.config['model']['num_layers'],
                collect_detailed=gate_config['collect_detailed']
            )
        else:
            self.gate_monitor = None
    
    def train_step(self, batch: Dict[str, torch.Tensor], step: int) -> Dict[str, float]:
        """å•æ­¥è®­ç»ƒ"""
        if self.rank == 0 and step <= 5:  # å‰5æ­¥æ˜¾ç¤ºè¯¦ç»†debug
            print(f"  ğŸ”§ [DEBUG] Step {step}: Setting model to train mode...")
        self.model.train()
        
        if self.rank == 0 and step <= 5:
            print(f"  ğŸ“Š [DEBUG] Step {step}: Moving batch data to device...")
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        labels = batch['labels'].to(self.device)
        
        if self.rank == 0 and step <= 5:
            print(f"  ğŸ§  [DEBUG] Step {step}: Starting model forward pass (shape: {input_ids.shape})...")
        
        # å‰å‘ä¼ æ’­
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            collect_gate_stats=True,
            return_dict=True
        )
        
        if self.rank == 0 and step <= 5:
            print(f"  âœ… [DEBUG] Step {step}: Forward pass completed!")
            print(f"  ğŸ“‰ [DEBUG] Step {step}: Computing loss...")
        
        # è®¡ç®—æŸå¤±
        loss_dict = self.loss_fn(
            logits=outputs['logits'],
            labels=labels,
            gate_stats=outputs.get('gate_stats', None)
        )
        
        loss = loss_dict['total_loss']
        
        if self.rank == 0 and step <= 5:
            print(f"  ğŸ”™ [DEBUG] Step {step}: Starting backward pass...")
        
        # âœ… å…³é”®ä¿®æ”¹ï¼šDeepSpeed ä¸ é DeepSpeed èµ°ä¸åŒè·¯å¾„
        use_deepspeed = self.config['system'].get('use_deepspeed', False)
        
        if use_deepspeed:
            if self.rank == 0 and step <= 5:
                print(f"  ğŸ”¥ [DEBUG] Step {step}: Using DeepSpeed backward...")
            
            # DeepSpeedè·¯å¾„ï¼šbackward & step éƒ½äº¤ç»™ DeepSpeed
            self.model.backward(loss)
            
            if self.rank == 0 and step <= 5:
                print(f"  ğŸš€ [DEBUG] Step {step}: DeepSpeed model step...")
            
            self.model.step()
            
            # è¿”å›æ˜¯å¦çœŸæ­£æ›´æ–°äº†å‚æ•°
            did_update = self.model.is_gradient_accumulation_boundary()
            
            if self.rank == 0 and step <= 5:
                print(f"  âœ… [DEBUG] Step {step}: DeepSpeed step completed, did_update: {did_update}")
        
        else:
            if self.rank == 0 and step <= 5:
                print(f"  ğŸ”¥ [DEBUG] Step {step}: Using PyTorch backward...")
            
            # çº¯ PyTorch è·¯å¾„
            loss.backward()
            
            if self.rank == 0 and step <= 5:
                print(f"  âœ‚ï¸ [DEBUG] Step {step}: Clipping gradients...")
            
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                self.config['training']['max_grad_norm']
            )
            
            if self.rank == 0 and step <= 5:
                print(f"  ğŸš€ [DEBUG] Step {step}: PyTorch optimizer step...")
            
            self.optimizer.step()
            self.optimizer.zero_grad()
                
            did_update = True
            
            if self.rank == 0 and step <= 5:
                print(f"  âœ… [DEBUG] Step {step}: PyTorch step completed")
        
        # æ›´æ–°é—¨æ§ç›‘æ§
        if self.gate_monitor is not None and outputs.get('gate_stats'):
            for layer_idx, gate_stats in enumerate(outputs['gate_stats']):
                self.gate_monitor.update(layer_idx, gate_stats)
        
        # è¿”å›æŸå¤±ç»Ÿè®¡å’Œæ˜¯å¦æ›´æ–°äº†å‚æ•°
        result = {k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}
        result['did_update'] = did_update
        return result
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        training_config = self.config['training']
        logging_config = self.config['logging']
        
        step = 0
        epoch = 0
        training_complete = False  # ğŸ”§ æ·»åŠ å…¨å±€åœè®­æ ‡å¿—
        
        logger.info("Starting training...")
        
        while step < training_config['max_steps'] and not training_complete:
            epoch += 1
            
            # è®¾ç½®åˆ†å¸ƒå¼é‡‡æ ·å™¨çš„epochï¼ˆå…³é”®ï¼šç¡®ä¿å„rankæ•°æ®ä¸€è‡´æ€§ï¼‰
            if (self.distributed and 
                hasattr(self.train_loader, 'sampler') and 
                hasattr(self.train_loader.sampler, 'set_epoch')):
                self.train_loader.sampler.set_epoch(epoch)
            
            # æ£€æŸ¥è¯¾ç¨‹å­¦ä¹ æ˜¯å¦éœ€è¦æ›´æ–°
            if hasattr(self.train_loader.dataset, 'step'):
                length_updated = self.train_loader.dataset.step()
                if length_updated:
                    logger.info(f"Curriculum updated at step {step}")
            
            progress_bar = tqdm(
                self.train_loader, 
                desc=f"Epoch {epoch}",
                disable=self.rank != 0
            )
            
            for batch_idx, batch in enumerate(progress_bar):
                if self.rank == 0 and step < 5:  # å‰5æ­¥æ˜¾ç¤ºè¯¦ç»†debug
                    print(f"ğŸ“¦ [DEBUG] Starting batch {batch_idx+1}, current step: {step}")
                
                # è®­ç»ƒæ­¥éª¤
                loss_dict = self.train_step(batch, step + 1)
                
                if self.rank == 0 and step < 5:
                    print(f"ğŸ“¦ [DEBUG] train_step completed for batch {batch_idx+1}")
                
                # âœ… åªæœ‰åœ¨çœŸæ­£æ›´æ–°å‚æ•°æ—¶æ‰å¢åŠ step
                did_update = loss_dict.pop('did_update', True)
                
                if did_update:
                    step += 1
                    if self.rank == 0:
                        print(f"âœ… Step {step}: Loss={loss_dict.get('total_loss', 'N/A'):.4f}")
                else:
                    if self.rank == 0 and step < 5:  # åªåœ¨å‰å‡ æ­¥æ˜¾ç¤ºç´¯ç§¯ä¿¡æ¯
                        print(f"â³ Accumulating gradients (step will be {step+1})")
                
                if self.rank == 0 and step < 5:
                    print(f"ğŸ“¦ [DEBUG] Finished processing batch {batch_idx+1}, moving to next...")
                
                # æ—¥å¿—è®°å½•ï¼ˆåªåœ¨çœŸæ­£æ›´æ–°æ—¶ï¼‰
                log_interval = logging_config.get('log_interval', 10)
                if did_update and step % log_interval == 0 and self.rank == 0:
                    # âœ… è·å–å­¦ä¹ ç‡ - å…¼å®¹DeepSpeedå’ŒPyTorch
                    use_deepspeed = self.config['system'].get('use_deepspeed', False)
                    if use_deepspeed:
                        lr = self.model.get_lr()[0] if hasattr(self.model, 'get_lr') else self.optimizer.param_groups[0]['lr']
                    else:
                        lr = self.optimizer.param_groups[0]['lr']
                    
                    log_msg = f"Step {step}/{training_config['max_steps']}, "
                    log_msg += f"LR: {lr:.2e}, "
                    log_msg += f"Loss: {loss_dict['total_loss']:.4f}"
                    
                    if 'load_balance_loss' in loss_dict:
                        log_msg += f", LB: {loss_dict['load_balance_loss']:.4f}"
                    if 'entropy_loss' in loss_dict:
                        log_msg += f", Ent: {loss_dict['entropy_loss']:.4f}"
                    
                    logger.info(log_msg)
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.set_postfix(loss=loss_dict['total_loss'])
                    
                    # wandbè®°å½•
                    if wandb.run is not None:
                        wandb.log({
                            'train/loss': loss_dict['total_loss'],
                            'train/lm_loss': loss_dict.get('lm_loss', 0),
                            'train/lb_loss': loss_dict.get('load_balance_loss', 0),
                            'train/entropy_loss': loss_dict.get('entropy_loss', 0),
                            'train/learning_rate': lr,
                            'train/step': step
                        })
                
                # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªåœ¨çœŸæ­£æ›´æ–°æ—¶ï¼‰
                save_interval = logging_config.get('save_interval', 100)
                if did_update and step % save_interval == 0 and self.rank == 0:
                    self._save_checkpoint(step)
                
                # é—¨æ§ç›‘æ§ä¿å­˜ï¼ˆåªåœ¨çœŸæ­£æ›´æ–°æ—¶ï¼‰
                if (did_update and self.gate_monitor is not None and 
                    step % self.config['gate_monitor']['save_interval'] == 0 and 
                    self.rank == 0):
                    save_path = self.config['gate_monitor']['save_path']
                    os.makedirs(os.path.dirname(save_path), exist_ok=True)
                    self.gate_monitor.save_statistics(save_path)
                
                # ğŸ”§ åˆ†å¸ƒå¼å®‰å…¨çš„åœè®­æœºåˆ¶ï¼šç¡®ä¿æ‰€æœ‰rankåŒæ—¶æ”¶åˆ°åœè®­ä¿¡å·
                if did_update:  # åªåœ¨å®Œæˆå®Œæ•´ä¼˜åŒ–æ­¥éª¤ååˆ¤æ–­
                    if self.distributed:
                        # åˆ›å»ºåœè®­æ ‡å¿—å¼ é‡
                        stop_flag = torch.tensor([0], device=self.device)
                        
                        # rank0åˆ¤æ–­æ˜¯å¦è¾¾åˆ°max_steps
                        if self.rank == 0 and step >= training_config['max_steps']:
                            stop_flag.fill_(1)
                            
                        # å¹¿æ’­åœè®­ä¿¡å·åˆ°æ‰€æœ‰rank
                        torch.distributed.broadcast(stop_flag, src=0)
                        
                                                # æ‰€æœ‰rankæ ¹æ®å¹¿æ’­ä¿¡å·å†³å®šæ˜¯å¦åœè®­
                        if stop_flag.item() == 1:
                            if self.rank == 0:
                                print(f"ğŸ” [DEBUG] Reached max_steps ({training_config['max_steps']}), broadcasting stop signal")
                            training_complete = True
                            break
                    else:
                        # éåˆ†å¸ƒå¼æ¨¡å¼ï¼šç›´æ¥åˆ¤æ–­
                        if step >= training_config['max_steps']:
                            training_complete = True
                            break
            
            # ğŸ”§ ç¡®ä¿tqdmè¿›åº¦æ¡æ­£ç¡®å…³é—­
            if hasattr(progress_bar, 'close'):
                progress_bar.close()
        
        # è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œé—¨æ§ç»Ÿè®¡
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ‰€æœ‰rankéƒ½å¿…é¡»è°ƒç”¨DeepSpeedä¿å­˜ï¼Œé¿å…barrieræ­»é”
        self._save_checkpoint(step, is_final=True)
        
        # åªæœ‰rank0å¤„ç†gate monitorå’Œå…¶ä»–æ–‡ä»¶æ“ä½œ
        if self.rank == 0:
            if self.gate_monitor is not None:
                # å¯¼å‡ºè£å‰ªè®¡åˆ’
                prune_plan = self.gate_monitor.export_prune_plan(
                    mamba_threshold_high=self.config['gate_monitor']['mamba_threshold_high'],
                    attention_threshold_low=self.config['gate_monitor']['attention_threshold_low'],
                    min_steps=self.config['gate_monitor']['min_steps_for_pruning']
                )
                
                plan_path = os.path.join(
                    self.config['logging']['output_dir'], 
                    'prune_plan.json'
                )
                import json
                with open(plan_path, 'w', encoding='utf-8') as f:
                    json.dump(prune_plan, f, indent=2, ensure_ascii=False)
                
                logger.info(f"Pruning plan saved to {plan_path}")
        
        # ğŸ”§ æ˜¾å¼åŒæ­¥ç¡®ä¿æ‰€æœ‰rankå®Œæˆä¿å­˜åå†æ‰“å°å®Œæˆä¿¡æ¯
        if hasattr(self, 'distributed') and self.distributed:
            torch.distributed.barrier()
            
        if self.rank == 0:
            logger.info("Training completed!")
    
    def _save_checkpoint(self, step: int, is_final: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        output_dir = self.config['logging']['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        
        use_deepspeed = self.config['system'].get('use_deepspeed', False)
        
        if use_deepspeed:
            # âœ… DeepSpeedä¿å­˜æ–¹å¼ - æ‰€æœ‰rankéƒ½å¿…é¡»è°ƒç”¨save_checkpoint
            tag = "final" if is_final else f"step_{step}"
            
            if self.rank == 0:
                logger.info(f"Saving DeepSpeed checkpoint: {tag}")
            
            # ğŸ”§ å…³é”®ï¼šæ‰€æœ‰rankéƒ½å¿…é¡»è°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼ˆå†…éƒ¨æœ‰barrierï¼‰
            self.model.save_checkpoint(output_dir, tag=tag)
            
            # ğŸ”§ åªæœ‰rank0ä¿å­˜é¢å¤–çš„é…ç½®æ–‡ä»¶ï¼Œé¿å…æ–‡ä»¶å†²çª
            if self.rank == 0:
                config_path = os.path.join(output_dir, tag, "config.json")
                with open(config_path, 'w') as f:
                    json.dump(self.config, f, indent=2)
                logger.info(f"âœ… DeepSpeed checkpoint saved: {output_dir}/{tag}")
                
        else:
            # âœ… PyTorchåŸç”Ÿä¿å­˜æ–¹å¼
            # è·å–æ¨¡å‹çŠ¶æ€ï¼ˆå¤„ç†DDPåŒ…è£…ï¼‰
            model_state = self.model.module.state_dict() if self.distributed else self.model.state_dict()
            
            checkpoint = {
                'step': step,
                'model_state_dict': model_state,
                'optimizer_state_dict': self.optimizer.state_dict(),
                'config': self.config
            }
            
            if self.scheduler is not None:
                checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if is_final:
                save_path = os.path.join(output_dir, 'final_model.pt')
            else:
                save_path = os.path.join(output_dir, f'checkpoint_step_{step}.pt')
            
            logger.info(f"Saving PyTorch checkpoint: {save_path}")
            torch.save(checkpoint, save_path)
        
        # ğŸ”§ ä¿å­˜æœ€ä½³æ¨¡å‹é“¾æ¥ï¼ˆåªåœ¨éDeepSpeedæ¨¡å¼æˆ–rank0ï¼‰
        if use_deepspeed:
            # DeepSpeedæ¨¡å¼ï¼šå®šä¹‰ä¿å­˜è·¯å¾„ç”¨äºé“¾æ¥åˆ›å»º
            tag = "final" if is_final else f"step_{step}"
            save_path = os.path.join(output_dir, tag)
            
            # åªæœ‰rank0åˆ›å»ºæœ€ä½³æ¨¡å‹é“¾æ¥ï¼Œé¿å…å†²çª
            if self.rank == 0:
                best_path = os.path.join(output_dir, 'best_model')
                if not os.path.exists(best_path) or is_final:
                    if os.path.exists(best_path):
                        os.remove(best_path) 
                    os.symlink(tag, best_path)  # é“¾æ¥åˆ°checkpointç›®å½•
        else:
            # PyTorchæ¨¡å¼ï¼šæ­£å¸¸åˆ›å»ºé“¾æ¥
            best_path = os.path.join(output_dir, 'best_model.pt')
            if not os.path.exists(best_path) or is_final:
                if os.path.exists(best_path):
                    os.remove(best_path)
                os.symlink(os.path.basename(save_path), best_path)
            
        # åªåœ¨rank0è®°å½•ä¿å­˜å®Œæˆæ—¥å¿—
        if self.rank == 0:
            if use_deepspeed:
                logger.info(f"âœ… DeepSpeed checkpoint and links created: {save_path}")
            else:
                logger.info(f"âœ… PyTorch checkpoint saved: {save_path}")


def main():
    parser = argparse.ArgumentParser(description="FusionTree Training")
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--distributed', action='store_true', help='æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå¼•æ“
    engine = TrainingEngine(args.config, args.distributed)
    
    # å¼€å§‹è®­ç»ƒ
    engine.train()


if __name__ == "__main__":
    main() 