"""
æ•°æ®å¤„ç†æ¨¡å—ï¼šé•¿æ–‡æ‹¼æ¥ã€maskã€æ®µè½æ ‡æ³¨
æ”¯æŒé•¿ä¸Šä¸‹æ–‡è®­ç»ƒçš„æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½
"""

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Optional, Any, Tuple
import json
import random
import numpy as np
import logging
from pathlib import Path
from functools import partial
import glob  # ğŸ”§ æ·»åŠ globæ”¯æŒé€šé…ç¬¦è·¯å¾„


logger = logging.getLogger(__name__)


class LongContextDataset(Dataset):
    """
    é•¿ä¸Šä¸‹æ–‡æ•°æ®é›†
    æ”¯æŒå¤šç§æ•°æ®æ ¼å¼å’Œé•¿åº¦è¯¾ç¨‹å­¦ä¹ 
    """
    
    def __init__(
        self,
        data_paths: List[str],
        tokenizer,
        max_length: int = 32768,
        min_length: int = 512,
        concat_docs: bool = True,
        add_special_tokens: bool = True,
        document_separator: str = "\n\n",
        enable_packing: bool = True,
        needle_in_haystack_prob: float = 0.0,
        structured_task_prob: float = 0.0
    ):
        """
        Args:
            data_paths: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            min_length: æœ€å°åºåˆ—é•¿åº¦
            concat_docs: æ˜¯å¦æ‹¼æ¥å¤šä¸ªæ–‡æ¡£
            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Štoken
            document_separator: æ–‡æ¡£åˆ†éš”ç¬¦
            enable_packing: æ˜¯å¦å¯ç”¨åºåˆ—æ‰“åŒ…
            needle_in_haystack_prob: needle-in-haystackä»»åŠ¡æ¦‚ç‡
            structured_task_prob: ç»“æ„åŒ–ä»»åŠ¡æ¦‚ç‡
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.min_length = min_length
        self.concat_docs = concat_docs
        self.add_special_tokens = add_special_tokens
        self.document_separator = document_separator
        self.enable_packing = enable_packing
        self.needle_in_haystack_prob = needle_in_haystack_prob
        self.structured_task_prob = structured_task_prob
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        self.documents = self._load_documents(data_paths)
        self.examples = self._prepare_examples()
        
        logger.info(f"Loaded {len(self.documents)} documents, created {len(self.examples)} examples")
    
    def _load_documents(self, data_paths: List[str]) -> List[Dict[str, Any]]:
        """åŠ è½½æ–‡æ¡£æ•°æ®"""
        documents = []
        logger = logging.getLogger(__name__)
        
        for path_pattern in data_paths:
            # ğŸ”§ ä½¿ç”¨globå±•å¼€é€šé…ç¬¦è·¯å¾„
            expanded_paths = glob.glob(str(path_pattern))
            
            if not expanded_paths:
                logger.warning(f"Data path {path_pattern} does not exist, skipping")
                continue
            
            logger.info(f"Found {len(expanded_paths)} files matching pattern: {path_pattern}")
            
            # å¤„ç†æ¯ä¸ªå±•å¼€çš„æ–‡ä»¶è·¯å¾„
            for file_path in expanded_paths:
                logger.info(f"Processing file: {file_path}")
                path = Path(file_path)
                
                if not path.exists():
                    logger.warning(f"File {path} does not exist, skipping")
                    continue
                
                try:
                    if path.suffix == '.jsonl':
                        with open(path, 'r', encoding='utf-8') as f:
                            for line_num, line in enumerate(f, 1):
                                if line_num % 10000 == 0:  # å‡å°‘æ—¥å¿—é¢‘ç‡
                                    logger.info(f"Processing line {line_num} of {file_path}")
                                if line.strip():
                                    try:
                                        doc = json.loads(line)
                                        documents.append(doc)
                                    except json.JSONDecodeError as e:
                                        logger.warning(f"Invalid JSON in {path}:{line_num}: {e}")
                                        continue
                    elif path.suffix == '.json':
                        with open(path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if isinstance(data, list):
                                documents.extend(data)
                            else:
                                documents.append(data)
                    else:
                        # çº¯æ–‡æœ¬æ–‡ä»¶
                        with open(path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            # æŒ‰æ®µè½åˆ†å‰²
                            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
                            for para in paragraphs:
                                documents.append({'text': para, 'source': str(path)})
                                
                except Exception as e:
                    logger.error(f"Error loading file {path}: {e}")
                    continue
        
        return documents
    
    def _prepare_examples(self) -> List[Dict[str, Any]]:
        """å‡†å¤‡è®­ç»ƒæ ·æœ¬"""
        examples = []
        
        if self.enable_packing:
            examples = self._pack_documents()
        else:
            # ç®€å•å¤„ç†ï¼šæ¯ä¸ªæ–‡æ¡£å•ç‹¬ä½œä¸ºä¸€ä¸ªæ ·æœ¬
            for doc in self.documents:
                text = doc.get('text', '')
                if len(text) < 50:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æ¡£
                    continue
                
                example = self._tokenize_text(text)
                if example['input_ids'].size(0) >= self.min_length:
                    examples.append(example)
        
        return examples
    
    def _pack_documents(self) -> List[Dict[str, Any]]:
        """å°†å¤šä¸ªæ–‡æ¡£æ‰“åŒ…åˆ°ä¸€ä¸ªæ ·æœ¬ä¸­"""
        examples = []
        current_text = ""
        current_docs = []
        
        for doc in self.documents:
            text = doc.get('text', '')
            if len(text) < 20:
                continue
            
            # å°è¯•æ·»åŠ å½“å‰æ–‡æ¡£
            test_text = current_text
            if test_text:
                test_text += self.document_separator
            test_text += text
            
            # æ£€æŸ¥æ˜¯å¦è¶…é•¿ - æ·»åŠ æˆªæ–­å‚æ•°é¿å…è­¦å‘Š
            test_tokens = self.tokenizer.encode(
                test_text, 
                add_special_tokens=False,
                max_length=self.max_length,
                truncation=True
            )
            
            if len(test_tokens) <= self.max_length:
                # å¯ä»¥æ·»åŠ 
                current_text = test_text
                current_docs.append(doc)
            else:
                # è¶…é•¿äº†ï¼Œä¿å­˜å½“å‰åºåˆ—å¹¶å¼€å§‹æ–°çš„
                if current_text and len(current_docs) > 0:
                    example = self._create_packed_example(current_text, current_docs)
                    if example:
                        examples.append(example)
                
                # å¼€å§‹æ–°åºåˆ— - å¦‚æœå•ä¸ªæ–‡æ¡£å°±è¶…é•¿ï¼Œä¹Ÿè¦æˆªæ–­
                if len(self.tokenizer.encode(text, add_special_tokens=False, max_length=self.max_length, truncation=True)) <= self.max_length:
                    current_text = text
                    current_docs = [doc]
                else:
                    # å•ä¸ªæ–‡æ¡£å°±è¶…é•¿ï¼Œåˆ›å»ºæˆªæ–­ç‰ˆæœ¬
                    truncated_example = self._tokenize_text(text)
                    if truncated_example['input_ids'].size(0) >= self.min_length:
                        truncated_example['num_docs'] = 1
                        truncated_example['doc_sources'] = [doc.get('source', 'unknown')]
                        examples.append(truncated_example)
                    current_text = ""
                    current_docs = []
        
        # å¤„ç†æœ€åä¸€ä¸ªåºåˆ—
        if current_text and len(current_docs) > 0:
            example = self._create_packed_example(current_text, current_docs)
            if example:
                examples.append(example)
        
        return examples
    
    def _create_packed_example(self, text: str, docs: List[Dict]) -> Optional[Dict[str, Any]]:
        """åˆ›å»ºæ‰“åŒ…æ ·æœ¬"""
        # éšæœºå†³å®šæ˜¯å¦åº”ç”¨ç‰¹æ®Šä»»åŠ¡
        if random.random() < self.needle_in_haystack_prob:
            text = self._add_needle_in_haystack(text)
        elif random.random() < self.structured_task_prob:
            text = self._add_structured_task(text, docs)
        
        example = self._tokenize_text(text)
        
        if example['input_ids'].size(0) < self.min_length:
            return None
        
        # æ·»åŠ å…ƒä¿¡æ¯
        example['num_docs'] = len(docs)
        example['doc_sources'] = [doc.get('source', 'unknown') for doc in docs]
        
        return example
    
    def _add_needle_in_haystack(self, text: str) -> str:
        """æ·»åŠ needle-in-haystackä»»åŠ¡"""
        # ç”Ÿæˆéšæœº"é’ˆ"
        needle_facts = [
            "The magic number is 42.",
            "The secret code is FUSION2024.",
            "The hidden treasure is buried at coordinates 123.456, 789.012.",
            "The password is: OpenSesame123!",
            "The answer to the ultimate question is forty-two."
        ]
        
        needle = random.choice(needle_facts)
        
        # éšæœºæ’å…¥ä½ç½®ï¼ˆä¸è¦å¤ªé å‰æˆ–å¤ªé åï¼‰
        sentences = text.split('.')
        if len(sentences) > 10:
            insert_pos = random.randint(len(sentences) // 4, 3 * len(sentences) // 4)
            sentences.insert(insert_pos, f" {needle}")
            text = '.'.join(sentences)
        
        # åœ¨æœ«å°¾æ·»åŠ é—®é¢˜
        question = f"\n\nQuestion: What is the magic number mentioned in the text above? Answer:"
        text += question
        
        return text
    
    def _add_structured_task(self, text: str, docs: List[Dict]) -> str:
        """æ·»åŠ ç»“æ„åŒ–ä»»åŠ¡ï¼ˆæŒ‡ä»£è§£æã€å¥æ³•æ ‡æ³¨ç­‰ï¼‰"""
        tasks = ['coreference', 'named_entity', 'sentiment']
        task = random.choice(tasks)
        
        if task == 'coreference':
            # ç®€å•çš„æŒ‡ä»£è§£æä»»åŠ¡
            text += "\n\nTask: Identify all pronouns in the text and their referents."
        elif task == 'named_entity':
            # å‘½åå®ä½“è¯†åˆ«
            text += "\n\nTask: List all named entities (persons, locations, organizations) mentioned in the text."
        elif task == 'sentiment':
            # æƒ…æ„Ÿåˆ†æ
            text += "\n\nTask: Analyze the overall sentiment of each paragraph in the text."
        
        return text
    
    def _tokenize_text(self, text: str) -> Dict[str, Any]:
        """tokenizeæ–‡æœ¬"""
        encoding = self.tokenizer(
            text,
            add_special_tokens=self.add_special_tokens,
            max_length=self.max_length,
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding.get('attention_mask', torch.ones_like(input_ids)).squeeze(0)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': input_ids.clone()  # è¯­è¨€å»ºæ¨¡ä»»åŠ¡
        }
    
    def __len__(self) -> int:
        return len(self.examples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        return self.examples[idx]
    
    def set_max_length(self, max_length: int):
        """åŠ¨æ€è°ƒæ•´æœ€å¤§é•¿åº¦ï¼ˆç”¨äºé•¿åº¦è¯¾ç¨‹å­¦ä¹ ï¼‰"""
        if max_length != self.max_length:
            self.max_length = max_length
            logger.info(f"Updated max_length to {max_length}, regenerating examples...")
            self.examples = self._prepare_examples()


class LengthCurriculumDataset(LongContextDataset):
    """
    æ”¯æŒé•¿åº¦è¯¾ç¨‹å­¦ä¹ çš„æ•°æ®é›†
    é€æ­¥å¢åŠ åºåˆ—é•¿åº¦ï¼š4Kâ†’8Kâ†’16Kâ†’32K
    """
    
    def __init__(self, curriculum_schedule: List[Tuple[int, int]], **kwargs):
        """
        Args:
            curriculum_schedule: [(length, steps), ...] é•¿åº¦è¯¾ç¨‹è®¡åˆ’
        """
        self.curriculum_schedule = curriculum_schedule
        self.current_stage = 0
        self.steps_in_stage = 0
        
        # ä»æœ€å°é•¿åº¦å¼€å§‹
        initial_length = curriculum_schedule[0][0]
        # ç¡®ä¿ä¸ä¼šæŠŠ max_length ä¼ ä¸¤æ¬¡
        kwargs = dict(kwargs)
        kwargs.pop('max_length', None)
        super().__init__(max_length=initial_length, **kwargs)
    
    def step(self) -> bool:
        """
        æ‰§è¡Œä¸€æ­¥è®­ç»ƒï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°é•¿åº¦
        
        Returns:
            æ˜¯å¦æ›´æ–°äº†é•¿åº¦
        """
        self.steps_in_stage += 1
        
        if self.current_stage < len(self.curriculum_schedule) - 1:
            current_length, current_steps = self.curriculum_schedule[self.current_stage]
            
            if self.steps_in_stage >= current_steps:
                # è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
                self.current_stage += 1
                self.steps_in_stage = 0
                new_length = self.curriculum_schedule[self.current_stage][0]
                
                logger.info(f"Curriculum: advancing to stage {self.current_stage}, length {new_length}")
                self.set_max_length(new_length)
                return True
        
        return False


def collate_fn(batch: List[Dict[str, torch.Tensor]], max_model_length: int = 1024) -> Dict[str, torch.Tensor]:
    """
    æ•°æ®æ‰¹å¤„ç†å‡½æ•°
    å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—ï¼Œè¿›è¡Œpadding
    """
    # è·å–æœ€å¤§é•¿åº¦ï¼Œä½†ä¸è¶…è¿‡æ¨¡å‹æœ€å¤§å¤„ç†èƒ½åŠ›
    max_length = max(item['input_ids'].size(0) for item in batch)
    if max_length > max_model_length:
        logger.warning(f"Batch contains sequences longer than max_model_length ({max_length} > {max_model_length}), truncating...")
        max_length = max_model_length
    
    batch_input_ids = []
    batch_attention_mask = []
    batch_labels = []
    
    for item in batch:
        input_ids = item['input_ids']
        attention_mask = item['attention_mask']
        labels = item['labels']
        
        # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œæˆªæ–­
        if input_ids.size(0) > max_length:
            input_ids = input_ids[:max_length]
            attention_mask = attention_mask[:max_length]
            labels = labels[:max_length]
        
        # å³padding
        pad_length = max_length - input_ids.size(0)
        if pad_length > 0:
            input_ids = F.pad(input_ids, (0, pad_length), value=0)
            attention_mask = F.pad(attention_mask, (0, pad_length), value=0)
            labels = F.pad(labels, (0, pad_length), value=-100)  # -100ä¼šè¢«ignore
        
        batch_input_ids.append(input_ids)
        batch_attention_mask.append(attention_mask)
        batch_labels.append(labels)
    
    return {
        'input_ids': torch.stack(batch_input_ids),
        'attention_mask': torch.stack(batch_attention_mask).to(torch.bool),
        'labels': torch.stack(batch_labels)
    }


def create_data_loader(
    data_paths: List[str],
    tokenizer,
    batch_size: int = 4,
    max_length: int = 32768,
    num_workers: int = 0,
    shuffle: bool = True,
    curriculum_schedule: Optional[List[Tuple[int, int]]] = None,
    pin_memory: bool = True,
    prefetch_factor: int = 2,
    distributed: bool = False,
    **dataset_kwargs
) -> DataLoader:
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨
    
    Args:
        data_paths: æ•°æ®æ–‡ä»¶è·¯å¾„
        tokenizer: åˆ†è¯å™¨
        batch_size: æ‰¹å¤§å°
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        shuffle: æ˜¯å¦æ‰“ä¹±
        curriculum_schedule: é•¿åº¦è¯¾ç¨‹è®¡åˆ’
        pin_memory: æ˜¯å¦pinå†…å­˜
        prefetch_factor: é¢„å–å› å­
        distributed: æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
        **dataset_kwargs: å…¶ä»–æ•°æ®é›†å‚æ•°
        
    Returns:
        DataLoaderå®ä¾‹
    """
    if curriculum_schedule is not None:
        dataset = LengthCurriculumDataset(
            data_paths=data_paths,
            tokenizer=tokenizer,
            max_length=max_length,
            curriculum_schedule=curriculum_schedule,
            **dataset_kwargs
        )
    else:
        dataset = LongContextDataset(
            data_paths=data_paths,
            tokenizer=tokenizer,
            max_length=max_length,
            **dataset_kwargs
        )
    
    # åˆ†å¸ƒå¼é‡‡æ ·å™¨é…ç½®
    sampler = None
    if distributed:
        from torch.utils.data.distributed import DistributedSampler
        sampler = DistributedSampler(
            dataset, 
            shuffle=shuffle, 
            drop_last=True  # å…³é”®ï¼šç¡®ä¿å„rankçš„batchå¤§å°ä¸€è‡´
        )
        shuffle = False  # ä½¿ç”¨sampleræ—¶ä¸èƒ½åŒæ—¶shuffle
    
    # åˆ†ç¦»DataLoaderå‚æ•°å’ŒDatasetå‚æ•°
    dataloader_kwargs = {
        'batch_size': batch_size,
        'shuffle': shuffle,
        'sampler': sampler,
        'num_workers': num_workers,
        'pin_memory': pin_memory,
        'drop_last': True,  # ç¡®ä¿æœ€åä¸€ä¸ªbatchå¤§å°ä¸€è‡´
        'collate_fn': partial(collate_fn, max_model_length=max_length)
    }
    
    # æ·»åŠ prefetch_factorï¼ˆä»…å½“num_workers > 0æ—¶ï¼‰
    if num_workers > 0:
        dataloader_kwargs['prefetch_factor'] = prefetch_factor
    
    return DataLoader(dataset, **dataloader_kwargs)


def create_evaluation_datasets(
    eval_data_paths: Dict[str, List[str]],
    tokenizer,
    max_length: int = 32768,
    **kwargs
) -> Dict[str, Dataset]:
    """
    åˆ›å»ºè¯„ä¼°æ•°æ®é›†
    
    Args:
        eval_data_paths: {"task_name": [data_paths]} æ ¼å¼çš„è¯„ä¼°æ•°æ®
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§é•¿åº¦
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ä»»åŠ¡ååˆ°æ•°æ®é›†çš„æ˜ å°„
    """
    eval_datasets = {}
    
    for task_name, paths in eval_data_paths.items():
        dataset = LongContextDataset(
            data_paths=paths,
            tokenizer=tokenizer,
            max_length=max_length,
            concat_docs=False,  # è¯„ä¼°æ—¶ä¸æ‹¼æ¥æ–‡æ¡£
            enable_packing=False,  # è¯„ä¼°æ—¶ä¸æ‰“åŒ…
            **kwargs
        )
        eval_datasets[task_name] = dataset
    
    return eval_datasets


# é¢„å®šä¹‰çš„é•¿åº¦è¯¾ç¨‹è®¡åˆ’
CURRICULUM_SCHEDULES = {
    'standard': [
        (4096, 5000),    # 4K, 5K steps
        (8192, 5000),    # 8K, 5K steps  
        (16384, 5000),   # 16K, 5K steps
        (32768, 10000),  # 32K, 10K steps
    ],
    'aggressive': [
        (2048, 2000),
        (4096, 3000),
        (8192, 3000),
        (16384, 4000),
        (32768, 8000),
    ],
    'conservative': [
        (4096, 10000),
        (8192, 10000),
        (16384, 10000),
        (32768, 20000),
        (65536, 10000),  # æœ€ç»ˆåˆ°64K
    ]
} 