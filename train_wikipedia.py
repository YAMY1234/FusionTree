#!/usr/bin/env python3
"""
Wikipediaæ•°æ®é›†è®­ç»ƒè„šæœ¬
ä½¿ç”¨çœŸå®Wikipediaæ•°æ®è¿›è¡Œæ··åˆæ¨¡å‹é¢„è®­ç»ƒ
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
import torch
import torch.distributed as dist

# ğŸš€ é˜²æ­¢NCCLè¶…æ—¶çš„ç¯å¢ƒå˜é‡è®¾ç½®ï¼ˆåœ¨importä»»ä½•åˆ†å¸ƒå¼ç›¸å…³æ¨¡å—å‰è®¾ç½®ï¼‰
os.environ.setdefault('NCCL_TIMEOUT', '1800')  # 30åˆ†é’Ÿè¶…æ—¶ï¼ˆé»˜è®¤10åˆ†é’Ÿï¼‰
os.environ.setdefault('NCCL_DEBUG', 'WARN')    # è°ƒè¯•æ¨¡å¼æ”¹ä¸ºWARNï¼Œå‡å°‘æ—¥å¿—å™ªéŸ³
os.environ.setdefault('TORCH_NCCL_BLOCKING_WAIT', '0')  # ç”Ÿäº§ç¯å¢ƒå…³é—­é˜»å¡ç­‰å¾…
os.environ.setdefault('NCCL_ASYNC_ERROR_HANDLING', '1')  # å¼‚æ­¥é”™è¯¯å¤„ç†
os.environ.setdefault('OMP_NUM_THREADS', '1')   # é¿å…CPUè¿‡åº¦è®¢é˜…
os.environ.setdefault('CUDA_LAUNCH_BLOCKING', '0')  # å¼‚æ­¥CUDAå¯åŠ¨

# é¡¹ç›®è·¯å¾„è®¾ç½®
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥è®­ç»ƒç›¸å…³æ¨¡å—
from train.engine import TrainingEngine
from utils.config import load_config
from utils.logger import setup_logger

def parse_args():
    parser = argparse.ArgumentParser(description="Wikipediaæ•°æ®é›†è®­ç»ƒ")
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_gpus', type=int, default=1, help='GPUæ•°é‡')
    parser.add_argument('--master_port', type=str, default='29500', help='åˆ†å¸ƒå¼è®­ç»ƒç«¯å£')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    return parser.parse_args()

def setup_distributed(num_gpus: int, master_port: str):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if num_gpus > 1:
        # è®¾ç½®åˆ†å¸ƒå¼ç›¸å…³ç¯å¢ƒå˜é‡
        os.environ.setdefault('MASTER_ADDR', 'localhost')
        os.environ.setdefault('MASTER_PORT', master_port)
        
        # ğŸš€ å¢åŠ åˆ†å¸ƒå¼è¶…æ—¶æ—¶é—´é˜²æ­¢ä¿å­˜æ£€æŸ¥ç‚¹æ—¶è¶…æ—¶
        from datetime import timedelta
        timeout = timedelta(seconds=1800)  # 30åˆ†é’Ÿ
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        if not dist.is_initialized():
            dist.init_process_group(
                backend='nccl',
                timeout=timeout,  # é˜²æ­¢NCCLè¶…æ—¶
                world_size=num_gpus,
                rank=int(os.environ.get('LOCAL_RANK', 0))
            )
            
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        torch.cuda.set_device(local_rank)
        
        return local_rank, dist.get_rank(), dist.get_world_size()
    else:
        return 0, 0, 1

def main():
    args = parse_args()
    
    # ğŸš€ è®¾ç½®æ›´è¯¦ç»†çš„NCCLè°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼ï¼‰
    if args.debug:
        os.environ['NCCL_DEBUG'] = 'INFO'
        os.environ['TORCH_NCCL_BLOCKING_WAIT'] = '1'  # è°ƒè¯•æ—¶å¼€å¯é˜»å¡ç­‰å¾…
        print("ğŸ”§ Debug mode enabled: NCCL_DEBUG=INFO, BLOCKING_WAIT=1")
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    local_rank, rank, world_size = setup_distributed(args.num_gpus, args.master_port)
    
    # è®¾ç½®æ—¥å¿—ï¼ˆåªåœ¨rank0è®¾ç½®æ–‡ä»¶æ—¥å¿—ï¼‰
    logger = setup_logger(
        name="wikipedia_train",
        level=logging.DEBUG if args.debug else logging.INFO,
        log_to_file=(rank == 0)
    )
    
    if rank == 0:
        logger.info("ğŸš€ Starting Wikipedia training...")
        logger.info(f"  ğŸ“Š GPUs: {args.num_gpus}")
        logger.info(f"  ğŸ“ Config: {args.config}")
        logger.info(f"  ğŸŒ World size: {world_size}, Rank: {rank}")
        logger.info(f"  ğŸ”§ NCCL timeout: {os.environ.get('NCCL_TIMEOUT', 'default')}s")
        
        # æ£€æŸ¥ä¿å­˜è·¯å¾„æ˜¯å¦ä¸ºç½‘ç»œç›˜ï¼ˆç®€å•å¯å‘å¼æ£€æŸ¥ï¼‰
        config = load_config(args.config)
        output_dir = config['logging']['output_dir']
        if any(indicator in output_dir.lower() for indicator in ['nfs', 'ceph', 'hdfs', 'gpfs']):
            logger.warning(f"âš ï¸ Checkpoint path appears to be network storage: {output_dir}")
            logger.warning("   Consider using local storage (/tmp, /local) for better I/O performance")
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # åˆ›å»ºè®­ç»ƒå¼•æ“
        engine = TrainingEngine(
            config=config,
            local_rank=local_rank,
            rank=rank,
            world_size=world_size
        )
        
        # å¼€å§‹è®­ç»ƒ
        engine.train(resume_path=args.resume)
        
    except Exception as e:
        logger.error(f"âŒ Training failed: {e}")
        if args.debug:
            import traceback
            logger.error(f"ğŸ” Traceback:\n{traceback.format_exc()}")
        raise
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        if dist.is_initialized():
            dist.destroy_process_group()
        
        if rank == 0:
            logger.info("âœ… Training script completed")

if __name__ == "__main__":
    main() 