# FusionTree: ä¸‹ä¸€ä»£æ··åˆæ¶æ„è¯­è¨€æ¨¡å‹

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![PyTorch 2.4+](https://img.shields.io/badge/PyTorch-2.4+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

FusionTree æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ··åˆæ¶æ„è¯­è¨€æ¨¡å‹ï¼Œå·§å¦™èåˆäº† **Mambaï¼ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰** å’Œ **Attention æœºåˆ¶** çš„ä¼˜åŠ¿ï¼Œä¸“é—¨é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡è¿›è¡Œæ·±åº¦ä¼˜åŒ–ã€‚é€šè¿‡å¹¶è¡ŒåŒåˆ†æ”¯è®¾è®¡å’Œæ™ºèƒ½é—¨æ§èåˆï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ æœ€æ–°ä¼˜åŒ– (2025å¹´æ›´æ–°)
- **SDPAé›†æˆ**: è‡ªåŠ¨é€‰æ‹©FlashAttention2åç«¯ï¼Œæ¿€æ´»å†…å­˜ä¼˜åŒ–é«˜è¾¾50%
- **é€‰æ‹©æ€§æ¢¯åº¦æ£€æŸ¥ç‚¹**: åªå¯¹Mamba/Attentionåˆ†æ”¯é‡è®¡ç®—ï¼ŒèŠ‚çœ30-50%æ˜¾å­˜
- **å…±äº«RoPEç¼“å­˜**: è·¨å±‚å…±äº«ä½ç½®ç¼–ç ï¼Œå‡å°‘å¸¸é©»æ˜¾å­˜1GB+
- **8Ã—A6000ä¼˜åŒ–é…ç½®**: é’ˆå¯¹48GBæ˜¾å­˜å¡ä¼˜åŒ–çš„è®­ç»ƒé…ç½®

### ğŸ—ï¸ æ··åˆæ¶æ„è®¾è®¡
- **Mambaåˆ†æ”¯**: é«˜æ•ˆå¤„ç†é•¿ç¨‹è¯­ä¹‰ä¾èµ–ï¼ŒO(L)å¤æ‚åº¦
- **Attentionåˆ†æ”¯**: å±€éƒ¨ç»†èŠ‚æ•è·ï¼Œæ»‘çª—+å…¨å±€å¤´æ··åˆè®¾è®¡
- **è½»é—¨æ§èåˆ**: é€é€šé“åŠ¨æ€æƒé‡ï¼Œè‡ªé€‚åº”åˆ†æ”¯é€‰æ‹©
- **ç»Ÿä¸€æ—¶é—´ç¼–ç (SRTE)**: ä¿è¯åŒåˆ†æ”¯æ—¶åºä¿¡æ¯ä¸€è‡´æ€§

### ğŸ“Š è®­ç»ƒåŸºç¡€è®¾æ–½
- **å¤šæ¨¡å¼æ•°æ®åŠ è½½**: Static/Lazy/HF-Streamingä¸‰ç§æ¨¡å¼
- **DeepSpeedé›†æˆ**: ZeRO-2/3æ”¯æŒï¼Œè‡ªåŠ¨æ¢¯åº¦ç´¯ç§¯
- **åˆ†å¸ƒå¼å‹å¥½**: NCCLåç«¯ï¼Œå®Œæ•´çš„checkpointåŒæ­¥
- **æ™ºèƒ½ç›‘æ§**: å®æ—¶é—¨æ§ç»Ÿè®¡ï¼Œè‡ªåŠ¨è£å‰ªè®¡åˆ’ç”Ÿæˆ

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
FusionTree/
â”œâ”€â”€ configs/                          # ğŸ”§ é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ pretrain_wikipedia.yaml       # Wikipediaé¢„è®­ç»ƒé…ç½®(æ¨è)
â”‚   â”œâ”€â”€ pretrain_optimized.yaml       # å°å‹æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ deepspeed_zero3.json         # DeepSpeedé…ç½®
â”œâ”€â”€ models/                           # ğŸ§  æ ¸å¿ƒæ¨¡å‹
â”‚   â”œâ”€â”€ hybrid_model.py              # å®Œæ•´è¯­è¨€æ¨¡å‹+é…ç½®ç±»
â”‚   â”œâ”€â”€ hybrid_block.py              # æ··åˆå—+SRTE+é—¨æ§
â”‚   â”œâ”€â”€ local_global_attn.py         # å±€éƒ¨/å…¨å±€æ³¨æ„åŠ›+RoPE
â”‚   â””â”€â”€ mamba_block.py               # MambaçŠ¶æ€ç©ºé—´æ¨¡å‹
â”œâ”€â”€ train/                            # ğŸ¯ è®­ç»ƒæ¡†æ¶
â”‚   â”œâ”€â”€ engine.py                    # è®­ç»ƒå¼•æ“+DeepSpeedé›†æˆ
â”‚   â”œâ”€â”€ data.py                      # æ•°æ®åŠ è½½+è¯¾ç¨‹å­¦ä¹ 
â”‚   â”œâ”€â”€ lazy_data.py                 # å¤§æ–‡ä»¶æ‡’åŠ è½½
â”‚   â”œâ”€â”€ streaming_data.py            # HFæµå¼æ•°æ®
â”‚   â”œâ”€â”€ losses.py                    # å¤åˆæŸå¤±å‡½æ•°
â”‚   â””â”€â”€ monitor_gate.py              # é—¨æ§ç›‘æ§+è£å‰ªè®¡åˆ’
â”œâ”€â”€ deploy/                           # ğŸš€ éƒ¨ç½²å·¥å…·
â”‚   â”œâ”€â”€ export_pruned.py             # æ¨¡å‹è£å‰ªå¯¼å‡º
â”‚   â””â”€â”€ runtime_stub.py              # æ¨ç†è¿è¡Œæ—¶
â”œâ”€â”€ docs/                             # ğŸ“š æŠ€æœ¯æ–‡æ¡£
â”œâ”€â”€ scripts/                          # ğŸ“œ è¿è¡Œè„šæœ¬
â””â”€â”€ environment.yml                   # ğŸ Condaç¯å¢ƒ
```

## ğŸ”¥ æ¶æ„è®¾è®¡å›¾

### ğŸ—ï¸ å®Œæ•´ç³»ç»Ÿæ¶æ„

ä»¥ä¸‹æ˜¯FusionTreeçš„å®Œæ•´ç³»ç»Ÿæ¶æ„ï¼Œå±•ç¤ºä»æ•°æ®å¤„ç†åˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨æµç¨‹ï¼š

```mermaid
graph TB
    subgraph "FusionTree å®Œæ•´ç³»ç»Ÿæ¶æ„"
        subgraph "æ•°æ®å¤„ç†å±‚"
            A1["JSONLæ–‡ä»¶"] --> A2["LazyDataset<br/>å¤§æ–‡ä»¶æ‡’åŠ è½½"]
            A3["HFæ•°æ®é›†"] --> A4["StreamingDataset<br/>åœ¨çº¿æµå¼åŠ è½½"]
            A5["é™æ€æ–‡ä»¶"] --> A6["LongContextDataset<br/>ä¼ ç»ŸåŠ è½½"]
            
            A2 --> A7["ç»Ÿä¸€CollateFunction<br/>PAD Tokenå¯¹é½"]
            A4 --> A7
            A6 --> A7
            
            A7 --> A8["DataLoader<br/>åˆ†å¸ƒå¼é‡‡æ ·å™¨"]
        end
        
        subgraph "è®­ç»ƒå¼•æ“å±‚"
            B1["TrainingEngine"] --> B2["DeepSpeedåˆå§‹åŒ–<br/>ZeRO-2/3é€‰æ‹©"]
            B2 --> B3["SDPAåç«¯å¯ç”¨<br/>FlashAttention2ä¼˜å…ˆ"]
            B3 --> B4["åˆ†å¸ƒå¼åŒæ­¥<br/>NCCL Backend"]
        end
        
        subgraph "æ¨¡å‹æ ¸å¿ƒå±‚"
            C1["HybridLanguageModel"] --> C2["å…±äº«ç»„ä»¶åˆå§‹åŒ–"]
            C2 --> C3["å…±äº«SRTE<br/>è·¨å±‚æ—¶é—´ç¼–ç "]
            C2 --> C4["å…±äº«RoPE<br/>åŠ¨æ€ä½ç½®ç¼“å­˜"]
            
            C1 --> C5["HybridBlock Stack"]
            
            subgraph "HybridBlockå†…éƒ¨ç»“æ„"
                D1["è¾“å…¥Token"] --> D2["è¯åµŒå…¥+Dropout"]
                D2 --> D3["å±‚å½’ä¸€åŒ–"]
                D3 --> D4["SRTEæ—¶é—´ç¼–ç è·å–"]
                
                D4 --> D5["åˆ†æ”¯è¾“å…¥å‡†å¤‡"]
                D5 --> D6["Mambaåˆ†æ”¯<br/>+æ—¶é—´ç¼–ç "]
                D5 --> D7["Attentionåˆ†æ”¯<br/>RoPEç¼–ç "]
                
                subgraph "Mambaåˆ†æ”¯è¯¦æƒ…"
                    E1["çŠ¶æ€ç©ºé—´æŠ•å½±"] --> E2["é€‰æ‹©æ€§æœºåˆ¶"]
                    E2 --> E3["å¹¶è¡Œæ‰«æç®—æ³•"]
                    E3 --> E4["è¾“å‡ºæŠ•å½±"]
                end
                
                subgraph "Attentionåˆ†æ”¯è¯¦æƒ…"
                    F1["QKVæŠ•å½±"] --> F2["å¤šå¤´åˆ†ç¦»"]
                    F2 --> F3["å±€éƒ¨å¤´x10<br/>æ»‘çª—256"]
                    F2 --> F4["å…¨å±€å¤´x2<br/>å®Œæ•´åºåˆ—"]
                    
                    F3 --> F5["SDPAå—åŒ–è®¡ç®—<br/>å†…å­˜ä¼˜åŒ–"]
                    F4 --> F6["SDPAå…¨å±€è®¡ç®—<br/>FlashAttention2"]
                    
                    F5 --> F7["å±€éƒ¨+å…¨å±€åˆå¹¶"]
                    F6 --> F7
                    F7 --> F8["è¾“å‡ºæŠ•å½±+Dropout"]
                end
                
                D6 --> E1
                D7 --> F1
                E4 --> G1["ç‰¹å¾å¯¹é½MLP"]
                F8 --> G1
                
                G1 --> G2["ä½ç§©é—¨æ§è®¡ç®—<br/>Hâ†’râ†’Hé€é€šé“"]
                G2 --> G3["åŠ¨æ€æƒé‡èåˆ<br/>alpha*Mamba + (1-alpha)*Attention"]
                G3 --> G4["èåˆæŠ•å½±"]
                G4 --> G5["å°å‹MLP<br/>2Hæ‰©å±•"]
                G5 --> G6["æ®‹å·®è¿æ¥"]
                G6 --> G7["è¾“å‡ºå±‚å½’ä¸€åŒ–"]
            end
        end
        
        subgraph "ä¼˜åŒ–ç­–ç•¥å±‚"
            H1["æ¢¯åº¦æ£€æŸ¥ç‚¹<br/>é€‰æ‹©æ€§é‡è®¡ç®—"] --> H2["åªå¯¹Mamba+Attention"]
            H3["SDPAè‡ªåŠ¨åç«¯"] --> H4["FlashAttention2<br/>Memory-Efficient<br/>Mathå›è½"]
            H5["DeepSpeed ZeRO"] --> H6["å‚æ•°åˆ†ç‰‡<br/>æ¢¯åº¦åˆ†ç‰‡<br/>ä¼˜åŒ–å™¨åˆ†ç‰‡"]
            H7["æ··åˆç²¾åº¦"] --> H8["BF16è®¡ç®—<br/>FP32ç´¯ç§¯"]
        end
        
        subgraph "æŸå¤±å‡½æ•°å±‚"
            I1["HybridModelLoss"] --> I2["è¯­è¨€å»ºæ¨¡æŸå¤±<br/>CrossEntropy"]
            I1 --> I3["è´Ÿè½½å‡è¡¡æŸå¤±<br/>é—¨æ§å‡å€¼çº¦æŸ"]
            I1 --> I4["ç†µæ­£åˆ™æŸå¤±<br/>é˜²æ­¢æåŒ–"]
            I1 --> I5["çŸ¥è¯†è’¸é¦æŸå¤±<br/>å¯é€‰"]
            
            I2 --> I6["æ€»æŸå¤±èšåˆ"]
            I3 --> I6
            I4 --> I6
            I5 --> I6
        end
        
        subgraph "ç›‘æ§ä¸éƒ¨ç½²å±‚"
            J1["GateMonitor"] --> J2["å®æ—¶é—¨æ§ç»Ÿè®¡<br/>å‡å€¼/æ–¹å·®/åˆ†å¸ƒ"]
            J2 --> J3["è£å‰ªè®¡åˆ’ç”Ÿæˆ<br/>é˜ˆå€¼åˆ¤æ–­"]
            J3 --> J4["é™æ€æ¨¡å‹å¯¼å‡º<br/>åˆ†æ”¯ç§»é™¤"]
            
            J5["Wandbé›†æˆ"] --> J6["è®­ç»ƒæ›²çº¿<br/>é—¨æ§çƒ­åŠ›å›¾<br/>æ€§èƒ½æŒ‡æ ‡"]
        end
        
        subgraph "æ¨ç†éƒ¨ç½²å±‚"
            K1["æ¨¡å‹æ£€æŸ¥ç‚¹"] --> K2["æƒé‡åŠ è½½"]
            K2 --> K3["è£å‰ªåº”ç”¨<br/>å¯é€‰"]
            K3 --> K4["æ¨ç†è¿è¡Œæ—¶"]
            K4 --> K5["KVç¼“å­˜<br/>å¢é‡ç”Ÿæˆ"]
        end
    end
    
    %% æ•°æ®æµè¿æ¥
    A8 --> B1
    B4 --> C1
    C5 --> G7
    G7 --> I1
    I6 --> H1
    H1 --> J1
    J4 --> K1
    
    %% æ ·å¼å®šä¹‰
    classDef dataLayer fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef engineLayer fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef modelLayer fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
    classDef optimLayer fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef monitorLayer fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    
    class A1,A2,A3,A4,A5,A6,A7,A8 dataLayer
    class B1,B2,B3,B4 engineLayer
    class C1,C2,C3,C4,C5,D1,D2,D3,D4,D5,D6,D7,E1,E2,E3,E4,F1,F2,F3,F4,F5,F6,F7,F8,G1,G2,G3,G4,G5,G6,G7 modelLayer
    class H1,H2,H3,H4,H5,H6,H7,H8,I1,I2,I3,I4,I5,I6 optimLayer
    class J1,J2,J3,J4,J5,J6,K1,K2,K3,K4,K5 monitorLayer
```

### ğŸ”„ è®­ç»ƒæµç¨‹å›¾

è¯¦ç»†å±•ç¤ºFusionTreeçš„å®Œæ•´è®­ç»ƒè¿‡ç¨‹ï¼Œä»ç¯å¢ƒåˆå§‹åŒ–åˆ°æ¨¡å‹éƒ¨ç½²ï¼š

```mermaid
flowchart TD
    subgraph "FusionTree è®­ç»ƒæµç¨‹"
        A["å¼€å§‹è®­ç»ƒ"] --> B["ç¯å¢ƒåˆå§‹åŒ–"]
        B --> C["é…ç½®æ–‡ä»¶åŠ è½½<br/>pretrain_wikipedia.yaml"]
        C --> D["åˆ†å¸ƒå¼è®¾ç½®<br/>torchrun --nproc_per_node=8"]
        
        D --> E["æ•°æ®å±‚åˆå§‹åŒ–"]
        E --> F["TokenizeråŠ è½½<br/>GPT-2 tokenizer"]
        F --> G["æ•°æ®é›†åˆ›å»º<br/>LazyDataset"]
        G --> H["DataLoaderè®¾ç½®<br/>batch_size=4, grad_accum=2"]
        
        H --> I["æ¨¡å‹æ¶æ„åˆå§‹åŒ–"]
        I --> J["å…±äº«ç»„ä»¶åˆ›å»º<br/>SRTE + RoPE"]
        J --> K["HybridBlock Stack<br/>18å±‚æ··åˆå—"]
        K --> L["è¯­è¨€å»ºæ¨¡å¤´<br/>è¯æ±‡è¡¨æ˜ å°„"]
        
        L --> M["DeepSpeedå¼•æ“åˆå§‹åŒ–"]
        M --> N["ZeRO-2é…ç½®<br/>å‚æ•°åˆ†ç‰‡"]
        N --> O["SDPAåç«¯å¯ç”¨<br/>FlashAttention2ä¼˜å…ˆ"]
        O --> P["æ¢¯åº¦æ£€æŸ¥ç‚¹é…ç½®<br/>é€‰æ‹©æ€§é‡è®¡ç®—"]
        
        P --> Q["å¼€å§‹è®­ç»ƒå¾ªç¯"]
        
        subgraph "å•æ­¥è®­ç»ƒè¿‡ç¨‹"
            Q1["æ•°æ®æ‰¹æ¬¡åŠ è½½<br/>åºåˆ—é•¿åº¦1024"] --> Q2["å‰å‘ä¼ æ’­"]
            
            subgraph "å‰å‘ä¼ æ’­è¯¦æƒ…"
                R1["è¯åµŒå…¥"] --> R2["HybridBlock-1"]
                R2 --> R3["..."]
                R3 --> R4["HybridBlock-18"]
                R4 --> R5["è¯­è¨€å»ºæ¨¡å¤´"]
                
                subgraph "HybridBlockå‰å‘"
                    S1["å±‚å½’ä¸€åŒ–"] --> S2["SRTEç¼–ç è·å–"]
                    S2 --> S3["Mambaåˆ†æ”¯è®¡ç®—"]
                    S2 --> S4["Attentionåˆ†æ”¯è®¡ç®—"]
                    S3 --> S5["ç‰¹å¾å¯¹é½MLP"]
                    S4 --> S5
                    S5 --> S6["é—¨æ§æƒé‡è®¡ç®—"]
                    S6 --> S7["åŠ¨æ€èåˆ"]
                    S7 --> S8["è¾“å‡ºæŠ•å½±+æ®‹å·®"]
                end
            end
            
            Q2 --> Q3["æŸå¤±è®¡ç®—"]
            
            subgraph "å¤åˆæŸå¤±"
                T1["è¯­è¨€å»ºæ¨¡æŸå¤±<br/>CrossEntropy"] --> T4["æ€»æŸå¤±"]
                T2["è´Ÿè½½å‡è¡¡æŸå¤±<br/>coeff=0.05"] --> T4
                T3["ç†µæ­£åˆ™æŸå¤±<br/>coeff=3e-4"] --> T4
            end
            
            Q3 --> Q4["åå‘ä¼ æ’­<br/>æ¢¯åº¦è®¡ç®—"]
            Q4 --> Q5["æ¢¯åº¦ç´¯ç§¯<br/>2æ­¥ç´¯ç§¯"]
            Q5 --> Q6{"æ˜¯å¦æ›´æ–°?<br/>step % grad_accum == 0"}
            
            Q6 -->|"æ˜¯"| Q7["æ¢¯åº¦è£å‰ª<br/>max_norm=1.0"]
            Q7 --> Q8["ä¼˜åŒ–å™¨æ›´æ–°<br/>AdamW"]
            Q8 --> Q9["å­¦ä¹ ç‡è°ƒåº¦<br/>Cosineè¡°å‡"]
            Q9 --> Q10["æ¢¯åº¦æ¸…é›¶"]
            
            Q6 -->|"å¦"| Q11["ç»§ç»­ç´¯ç§¯"]
            Q11 --> Q1
            Q10 --> Q12["é—¨æ§ç»Ÿè®¡æ”¶é›†"]
        end
        
        Q --> Q1
        Q12 --> U["æ—¥å¿—è®°å½•ä¸ç›‘æ§"]
        
        subgraph "ç›‘æ§ä¸æ£€æŸ¥ç‚¹"
            U --> V{"step % log_interval == 0?"}
            V -->|"æ˜¯"| W["æ‰“å°è®­ç»ƒæŒ‡æ ‡<br/>LM loss, gate_mean, LR"]
            V -->|"å¦"| X
            
            W --> X{"step % save_interval == 0?"}
            X -->|"æ˜¯"| Y["ä¿å­˜æ£€æŸ¥ç‚¹<br/>åŒæ­¥barrier"]
            X -->|"å¦"| Z
            
            Y --> Y1["DeepSpeed checkpointä¿å­˜<br/>gather_16bit_weights=False"]
            Y1 --> Y2["é—¨æ§ç»Ÿè®¡ä¿å­˜<br/>è£å‰ªè®¡åˆ’æ›´æ–°"]
            Y2 --> Z
            
            Z{"step >= max_steps?"}
            Z -->|"å¦"| Q1
            Z -->|"æ˜¯"| AA["è®­ç»ƒå®Œæˆ"]
        end
        
        AA --> BB["æœ€ç»ˆæ£€æŸ¥ç‚¹ä¿å­˜"]
        BB --> CC["é—¨æ§ç»Ÿè®¡åˆ†æ"]
        CC --> DD["è£å‰ªè®¡åˆ’ç”Ÿæˆ"]
        DD --> EE["æ¨¡å‹å¯¼å‡º<br/>é™æ€/è£å‰ªç‰ˆæœ¬"]
    end
    
    %% æ ·å¼å®šä¹‰
    classDef initStep fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef dataStep fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef modelStep fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef trainStep fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef monitorStep fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    
    class A,B,C,D initStep
    class E,F,G,H dataStep
    class I,J,K,L,M,N,O,P modelStep
    class Q,Q1,Q2,Q3,Q4,Q5,Q6,Q7,Q8,Q9,Q10,Q11,Q12,R1,R2,R3,R4,R5,S1,S2,S3,S4,S5,S6,S7,S8,T1,T2,T3,T4 trainStep
    class U,V,W,X,Y,Y1,Y2,Z,AA,BB,CC,DD,EE monitorStep
```

### ğŸ”— ç»„ä»¶å…³ç³»å›¾

å±•ç¤ºFusionTreeå„æ¨¡å—ä¹‹é—´çš„ä¾èµ–å…³ç³»å’Œæ•°æ®æµï¼š

```mermaid
graph LR
    subgraph "FusionTree ç»„ä»¶å…³ç³»å›¾"
        subgraph "é…ç½®å±‚"
            A["configs/<br/>pretrain_wikipedia.yaml"]
            B["environment.yml<br/>ä¾èµ–ç®¡ç†"]
        end
        
        subgraph "æ•°æ®å±‚"
            C["train/data.py<br/>LongContextDataset"]
            D["train/lazy_data.py<br/>LazyDataset"]  
            E["train/streaming_data.py<br/>StreamingDataset"]
            
            C --> F["ç»Ÿä¸€CollateFunction"]
            D --> F
            E --> F
        end
        
        subgraph "æ¨¡å‹æ ¸å¿ƒ"
            G["models/hybrid_model.py<br/>HybridLanguageModel"]
            H["models/hybrid_block.py<br/>HybridBlock + SRTE"]
            I["models/mamba_block.py<br/>MambaBlock"]
            J["models/local_global_attn.py<br/>LocalGlobalAttention + RoPE"]
            
            G --> H
            H --> I
            H --> J
            H --> K["å…±äº«SRTE"]
            G --> L["å…±äº«RoPE"]
            J --> L
        end
        
        subgraph "è®­ç»ƒå¼•æ“"
            M["train/engine.py<br/>TrainingEngine"]
            N["train/losses.py<br/>HybridModelLoss"]
            O["train/monitor_gate.py<br/>GateMonitor"]
            
            M --> N
            M --> O
        end
        
        subgraph "ä¼˜åŒ–ç»„ä»¶"
            P["DeepSpeed ZeRO-2<br/>å‚æ•°åˆ†ç‰‡"]
            Q["SDPA Backend<br/>FlashAttention2"]
            R["Gradient Checkpointing<br/>é€‰æ‹©æ€§é‡è®¡ç®—"]
            S["Mixed Precision<br/>BF16è®­ç»ƒ"]
        end
        
        subgraph "éƒ¨ç½²å·¥å…·"
            T["deploy/export_pruned.py<br/>æ¨¡å‹è£å‰ª"]
            U["deploy/runtime_stub.py<br/>æ¨ç†è¿è¡Œæ—¶"]
            
            T --> V["é™æ€æ¨¡å‹"]
            U --> W["KVç¼“å­˜æ¨ç†"]
        end
    end
    
    %% ä¾èµ–å…³ç³»
    A --> M
    B --> M
    F --> M
    G --> M
    P --> M
    Q --> J
    R --> H
    S --> M
    O --> T
    
    %% æ•°æ®æµ
    M -.->|"è®­ç»ƒæ•°æ®"| F
    M -.->|"æ¨¡å‹å‚æ•°"| G
    M -.->|"æŸå¤±è®¡ç®—"| N
    M -.->|"é—¨æ§ç»Ÿè®¡"| O
    N -.->|"æ¢¯åº¦"| M
    O -.->|"è£å‰ªè®¡åˆ’"| T
    
    %% æ ·å¼
    classDef configLayer fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef dataLayer fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px  
    classDef modelLayer fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef trainLayer fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    classDef optimLayer fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef deployLayer fill:#e0f2f1,stroke:#00695c,stroke-width:2px
    
    class A,B configLayer
    class C,D,E,F dataLayer
    class G,H,I,J,K,L modelLayer
    class M,N,O trainLayer
    class P,Q,R,S optimLayer
    class T,U,V,W deployLayer
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <repository_url>
cd FusionTree

# åˆ›å»ºCondaç¯å¢ƒ
conda env create -f environment.yml
conda activate fusiontree-gpu

# æˆ–ä½¿ç”¨è„šæœ¬ä¸€é”®é…ç½®
source env_setup.sh
```

### 2. æ•°æ®å‡†å¤‡

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/train data/eval

# Wikipediaæ•°æ®ç¤ºä¾‹
echo '{"text": "Wikipediaæ˜¯ä¸€ä¸ªè‡ªç”±å†…å®¹ã€å…¬å¼€ç¼–è¾‘ä¸”å¤šè¯­è¨€çš„ç½‘ç»œç™¾ç§‘å…¨ä¹¦..."}' > data/train/wiki_sample.jsonl

# æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
# - JSONL: {"text": "content"}
# - é€šé…ç¬¦è·¯å¾„: data/train/*.jsonl
# - HuggingFaceæ•°æ®é›†æµå¼åŠ è½½
```

### 3. æ¨¡å‹è®­ç»ƒ

```bash
# ğŸ¯ æ¨èé…ç½®ï¼šWikipediaæ•°æ® (8Ã—A6000)
torchrun --nproc_per_node=8 train/engine.py \
  --config configs/pretrain_wikipedia.yaml \
  --distributed

# ğŸ”§ å°å‹æ¨¡å‹è°ƒè¯•
python train/engine.py --config configs/pretrain_optimized.yaml

# ğŸ“Š è®­ç»ƒç›‘æ§
# - Wandb: è®­ç»ƒæ›²çº¿ã€é—¨æ§ç»Ÿè®¡
# - æ—¥å¿—: lossä¸‹é™ã€gate_meanå˜åŒ–
# - æ˜¾å­˜: æ¯å¡<35GB (å……åˆ†åˆ©ç”¨48GB)
```

### 4. æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²

```bash
# æ¨¡å‹è¯„ä¼°
python eval/eval_llm.py --model_path checkpoints/step_5000

# è£å‰ªæ¨¡å‹å¯¼å‡º
python deploy/export_pruned.py \
  --model_path checkpoints/step_5000 \
  --prune_plan checkpoints/prune_plan.json \
  --output_dir deployed_models/
```

## ğŸ“Š æ ¸å¿ƒç®—æ³•è¯¦è§£

### HybridBlock å‰å‘ä¼ æ’­

```python
def forward(self, hidden_states, attention_mask=None, training=True):
    # 1. è¾“å…¥å½’ä¸€åŒ–
    normalized_input = self.ln_input(hidden_states)
    
    # 2. ç»Ÿä¸€æ—¶é—´ç¼–ç 
    time_encoding = self.srte(seq_len).to(hidden_states.dtype)
    
    # 3. åˆ†æ”¯è¾“å…¥å‡†å¤‡
    mamba_input = normalized_input + time_encoding  # Mambaç”¨SRTE
    attn_input = normalized_input                   # Attentionç”¨RoPE
    
    # 4. å¹¶è¡ŒåŒåˆ†æ”¯è®¡ç®— (æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹)
    if self.gradient_checkpointing and training:
        h_mamba = checkpoint(lambda x: self.mamba(x)[0], mamba_input)
        h_attn = checkpoint(lambda x: self.attention(x)[0], attn_input)
    else:
        h_mamba, _ = self.mamba(mamba_input, state=mamba_state)
        h_attn, _ = self.attention(attn_input, attention_mask=attention_mask)
    
    # 5. ç‰¹å¾å¯¹é½ä¸é—¨æ§èåˆ
    aligned_features = self.alignment(h_mamba, h_attn)
    gate_weights = self.gate(aligned_features)  # é€é€šé“æƒé‡
    fused_features = gate_weights * h_mamba + (1 - gate_weights) * h_attn
    
    # 6. è¾“å‡ºæŠ•å½±ä¸æ®‹å·®
    projected = self.fusion_proj(fused_features)
    mlp_output = self.small_mlp(aligned_features)
    output = hidden_states + projected + mlp_output
    
    return self.ln_output(output)
```

### SDPAä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶

```python
# å…¨å±€æ³¨æ„åŠ›ï¼šä½¿ç”¨PyTorch 2.4 SDPA
def global_attention(q, k, v, attention_mask=None):
    attn_mask = attention_mask[:, None, None, :] if attention_mask else None
    return F.scaled_dot_product_attention(
        q, k, v, 
        attn_mask=attn_mask, 
        is_causal=True  # è‡ªåŠ¨å› æœæ©ç ï¼Œæ›¿ä»£æ‰‹å·¥å®ç°
    )

# æ»‘çª—æ³¨æ„åŠ›ï¼šå—åŒ–+SDPA
def sliding_window_attention(q, k, v, window_size=256):
    for start_idx in range(0, seq_len, block_size):
        # è®¡ç®—æ»‘çª—èŒƒå›´
        k_start = max(0, start_idx - window_size // 2)
        k_end = min(seq_len, end_idx + window_size // 2)
        
        # å—å†…SDPAè®¡ç®—
        output_block = F.scaled_dot_product_attention(
            q_block, k_block, v_block, is_causal=True
        )
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–ç­–ç•¥

| ä¼˜åŒ–æŠ€æœ¯ | æ˜¾å­˜èŠ‚çœ | è¯´æ˜ |
|---------|---------|------|
| **å…±äº«RoPEç¼“å­˜** | ~1GB | 18å±‚å…±äº«cos/sinè¡¨ï¼ŒåŠ¨æ€æ‰©å®¹ |
| **é€‰æ‹©æ€§Checkpoint** | 30-50% | ä»…å¯¹Mamba/Attentioné‡è®¡ç®— |
| **SDPAåç«¯** | 20-30% | è‡ªåŠ¨é€‰æ‹©FlashAttention2å†…æ ¸ |
| **ZeRO-2åˆ†ç‰‡** | 2-8x | å‚æ•°åˆ†ç‰‡ï¼Œé€šä¿¡å¼€é”€æ›´å° |

### 8Ã—A6000 (48GB/å¡) æ¨èé…ç½®

```yaml
# configs/pretrain_wikipedia.yaml
training:
  batch_size: 4                    # æ¯å¡micro-batch
  gradient_accumulation_steps: 2   # æ€»batch=4Ã—8Ã—2Ã—1024=65k tokens
  max_seq_length: 1024             # å®é™…è®­ç»ƒé•¿åº¦
  gradient_checkpointing: true     # å¯ç”¨é€‰æ‹©æ€§é‡è®¡ç®—

model:
  hidden_size: 1536               # å¹³è¡¡ç²¾åº¦ä¸æ•ˆç‡
  num_layers: 18                  # é€‚ä¸­æ·±åº¦
  window_size: 256                # å±€éƒ¨çª—å£å¤§å°
  global_heads: 2                 # ä¿æŒé•¿ç¨‹å»ºæ¨¡
  max_position_embeddings: 8192   # RoPEç¼“å­˜å¤§å°

system:
  zero_stage: 2                   # ZeRO-2è·å¾—æ›´å¥½åå
  offload_optimizer: false        # å…³é—­offloadé¿å…I/Oç“¶é¢ˆ
  offload_params: false
```

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡

**è®­ç»ƒé˜¶æ®µ** (5000æ­¥ï¼ŒWikipediaæ•°æ®)ï¼š
- **LM Loss**: 7.5 â†’ 5.5-6.2
- **é—¨æ§ç»Ÿè®¡**: gate_mean 0.45-0.55ï¼Œstd 0.15â†’0.08-0.12
- **æ˜¾å­˜ä½¿ç”¨**: <35GB/å¡ (48GBå¡æœ‰å……åˆ†ä½™é‡)
- **ååæå‡**: æ¯”ZeRO-3 + micro-batch=2å¿«20-30%

**æ¨ç†é˜¶æ®µ** (è£å‰ªå)ï¼š
- **Prefill**: â‰¥90% åŸºçº¿åå
- **Decode**: +37% é€Ÿåº¦æå‡
- **æ˜¾å­˜èŠ‚çœ**: 22% å‡å°‘

## ğŸ¯ è®­ç»ƒç›‘æ§

### å…³é”®è§‚æµ‹æŒ‡æ ‡

```python
# 1. æŸå¤±æ›²çº¿å¥åº·æ£€æŸ¥
if step % 100 == 0:
    print(f"Step {step}: LM_loss={lm_loss:.4f}, Gate_mean={gate_mean:.3f}")

# 2. é—¨æ§åˆ†å¸ƒæ¼”åŒ–
# - æ—©æœŸ: gate_mean~0.5, é«˜std
# - ä¸­æœŸ: é€æ­¥åˆ†åŒ–ï¼Œå±‚é—´å·®å¼‚æ˜¾ç°
# - åæœŸ: ç¨³å®šåå‘ï¼ŒæŸäº›å±‚åMamba/Attention

# 3. æ­£åˆ™åŒ–æ•ˆæœ (ä¿®å¤detachå)
# - load_balance_loss: çº¦æŸgate_meanæ¥è¿‘ç›®æ ‡å€¼
# - entropy_loss: é˜²æ­¢é—¨æ§è¿‡æ—©æ”¶æ•›åˆ°æå€¼
```

### æ•…éšœæ’æŸ¥

| é—®é¢˜ç°è±¡ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|---------|---------|
| é—¨æ§é•¿æœŸè´´0.5ä¸å˜ | æ­£åˆ™æƒé‡è¿‡å° | è°ƒé«˜`entropy_reg_coeff` |
| é—¨æ§å¿«é€Ÿæ”¶æ•›åˆ°æå€¼ | è´Ÿè½½å‡è¡¡è¿‡å¼º | è°ƒä½`load_balance_coeff` |
| Lossä¸ä¸‹é™ | æ•°æ®/æ¢¯åº¦é—®é¢˜ | æ£€æŸ¥detachä¿®å¤ |
| NCCLè¶…æ—¶ | ä¿å­˜æ—¶èšåˆ | ç¡®è®¤`gather_16bit_weights_on_model_save: false` |

## ğŸ”¬ æŠ€æœ¯åŸç†

### Mamba vs Attention äº’è¡¥æ€§

```
é•¿ç¨‹ä¾èµ–å»ºæ¨¡:
â”œâ”€â”€ Mambaåˆ†æ”¯: O(L)å¤æ‚åº¦ï¼Œæ“…é•¿åºåˆ—çº§è¯­ä¹‰
â”‚   â”œâ”€â”€ çŠ¶æ€ç©ºé—´å»ºæ¨¡
â”‚   â”œâ”€â”€ é€‰æ‹©æ€§æœºåˆ¶
â”‚   â””â”€â”€ å¹¶è¡Œæ‰«æè®­ç»ƒ
â””â”€â”€ Attentionåˆ†æ”¯: O(LÂ²)â†’O(LÃ—W)ï¼Œæ“…é•¿å±€éƒ¨ç»†èŠ‚
    â”œâ”€â”€ æ»‘çª—å±€éƒ¨æ³¨æ„åŠ› (å¤§éƒ¨åˆ†å¤´)
    â”œâ”€â”€ å…¨å±€æ³¨æ„åŠ› (å°‘é‡å¤´)
    â””â”€â”€ SDPAå†…æ ¸ä¼˜åŒ–
```

### é—¨æ§èåˆæœºåˆ¶

```python
# ä½ç§©é—¨æ§: Hâ†’râ†’Hï¼Œå‡å°‘å‚æ•°é‡
class GateLowRank(nn.Module):
    def __init__(self, hidden_size, rank=96):
        self.u = nn.Linear(hidden_size, rank, bias=False)
        self.d = nn.Linear(rank, hidden_size, bias=False)
    
    def forward(self, z):
        return torch.sigmoid(self.d(torch.tanh(self.u(z))))

# é€é€šé“èåˆ: ç»†ç²’åº¦æ§åˆ¶
fused = gate_weights * h_mamba + (1 - gate_weights) * h_attn
```

## ğŸ“š ç›¸å…³å·¥ä½œ

- **[Mamba](https://arxiv.org/abs/2312.00752)**: çŠ¶æ€ç©ºé—´æ¨¡å‹çš„çªç ´æ€§è¿›å±•
- **[FlashAttention](https://arxiv.org/abs/2205.14135)**: é«˜æ•ˆæ³¨æ„åŠ›å®ç°
- **[RoPE](https://arxiv.org/abs/2104.09864)**: æ—‹è½¬ä½ç½®ç¼–ç 
- **[DeepSpeed](https://github.com/microsoft/DeepSpeed)**: å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. **ä»£ç é£æ ¼**: éµå¾ªPEP 8ï¼Œä½¿ç”¨type hints
2. **æµ‹è¯•**: æ–°åŠŸèƒ½éœ€è¦å•å…ƒæµ‹è¯•
3. **æ–‡æ¡£**: é‡è¦å‡½æ•°æ·»åŠ docstring
4. **æ€§èƒ½**: å…³æ³¨æ˜¾å­˜å’Œé€Ÿåº¦ä¼˜åŒ–

```bash
# å¼€å‘ç¯å¢ƒé…ç½®
git clone <your-fork>
cd FusionTree
pre-commit install  # ä»£ç æ ¼å¼æ£€æŸ¥
pytest tests/       # è¿è¡Œæµ‹è¯•å¥—ä»¶
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ä¸ºFusionTreeæä¾›çš„æŠ€æœ¯åŸºç¡€ï¼š
- **Mamba**: é©å‘½æ€§çš„çŠ¶æ€ç©ºé—´æ¨¡å‹æ¶æ„
- **PyTorch**: æ·±åº¦å­¦ä¹ æ¡†æ¶å’ŒSDPAä¼˜åŒ–
- **DeepSpeed**: å¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒè§£å†³æ–¹æ¡ˆ
- **FlashAttention**: é«˜æ•ˆæ³¨æ„åŠ›ç®—æ³•

---

â­ **å¦‚æœFusionTreeå¯¹æ‚¨çš„ç ”ç©¶æˆ–é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªStarï¼**

ğŸ“§ **è”ç³»æˆ‘ä»¬**: [Issues](../../issues) | [Discussions](../../discussions) 