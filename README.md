# FusionTree: ä¸‹ä¸€ä»£æ··åˆæ¶æ„è¯­è¨€æ¨¡å‹

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![PyTorch 2.4+](https://img.shields.io/badge/PyTorch-2.4+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

FusionTree æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ··åˆæ¶æ„è¯­è¨€æ¨¡å‹ï¼Œå·§å¦™èåˆäº† **Mambaï¼ˆçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼‰** å’Œ **Attention æœºåˆ¶** çš„ä¼˜åŠ¿ï¼Œä¸“é—¨é’ˆå¯¹é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡è¿›è¡Œæ·±åº¦ä¼˜åŒ–ã€‚é€šè¿‡å¹¶è¡ŒåŒåˆ†æ”¯è®¾è®¡å’Œæ™ºèƒ½é—¨æ§èåˆï¼Œåœ¨ä¿æŒé«˜ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡äº†è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ æœ€æ–°ä¼˜åŒ– (2025å¹´æ›´æ–°)
- **SDPAé›†æˆ**: è‡ªåŠ¨é€‰æ‹©FlashAttention2åç«¯ï¼Œæ¿€æ´»å†…å­˜ä¼˜åŒ–é«˜è¾¾50%
- **é€‰æ‹©æ€§æ¢¯åº¦æ£€æŸ¥ç‚¹**: åªå¯¹Mamba/Attentionåˆ†æ”¯é‡è®¡ç®—ï¼ŒèŠ‚çœ30-50%æ˜¾å­˜
- **å…±äº«RoPEç¼“å­˜**: è·¨å±‚å…±äº«ä½ç½®ç¼–ç ï¼Œå‡å°‘å¸¸é©»æ˜¾å­˜1GB+
- **8Ã—A6000ä¼˜åŒ–é…ç½®**: é’ˆå¯¹48GBæ˜¾å­˜å¡ä¼˜åŒ–çš„è®­ç»ƒé…ç½®

### ğŸ—ï¸ æ··åˆæ¶æ„è®¾è®¡
- **Mambaåˆ†æ”¯**: é«˜æ•ˆå¤„ç†é•¿ç¨‹è¯­ä¹‰ä¾èµ–ï¼ŒO(L)å¤æ‚åº¦
- **Attentionåˆ†æ”¯**: å±€éƒ¨ç»†èŠ‚æ•è·ï¼Œæ»‘çª—+å…¨å±€å¤´æ··åˆè®¾è®¡
- **è½»é—¨æ§èåˆ**: é€é€šé“åŠ¨æ€æƒé‡ï¼Œè‡ªé€‚åº”åˆ†æ”¯é€‰æ‹©
- **ç»Ÿä¸€æ—¶é—´ç¼–ç (SRTE)**: ä¿è¯åŒåˆ†æ”¯æ—¶åºä¿¡æ¯ä¸€è‡´æ€§

### ğŸ“Š è®­ç»ƒåŸºç¡€è®¾æ–½
- **å¤šæ¨¡å¼æ•°æ®åŠ è½½**: Static/Lazy/HF-Streamingä¸‰ç§æ¨¡å¼
- **DeepSpeedé›†æˆ**: ZeRO-2/3æ”¯æŒï¼Œè‡ªåŠ¨æ¢¯åº¦ç´¯ç§¯
- **åˆ†å¸ƒå¼å‹å¥½**: NCCLåç«¯ï¼Œå®Œæ•´çš„checkpointåŒæ­¥
- **æ™ºèƒ½ç›‘æ§**: å®æ—¶é—¨æ§ç»Ÿè®¡ï¼Œè‡ªåŠ¨è£å‰ªè®¡åˆ’ç”Ÿæˆ

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
FusionTree/
â”œâ”€â”€ configs/                          # ğŸ”§ é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ pretrain_wikipedia.yaml       # Wikipediaé¢„è®­ç»ƒé…ç½®(æ¨è)
â”‚   â”œâ”€â”€ pretrain_optimized.yaml       # å°å‹æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ deepspeed_zero3.json         # DeepSpeedé…ç½®
â”œâ”€â”€ models/                           # ğŸ§  æ ¸å¿ƒæ¨¡å‹
â”‚   â”œâ”€â”€ hybrid_model.py              # å®Œæ•´è¯­è¨€æ¨¡å‹+é…ç½®ç±»
â”‚   â”œâ”€â”€ hybrid_block.py              # æ··åˆå—+SRTE+é—¨æ§
â”‚   â”œâ”€â”€ local_global_attn.py         # å±€éƒ¨/å…¨å±€æ³¨æ„åŠ›+RoPE
â”‚   â””â”€â”€ mamba_block.py               # MambaçŠ¶æ€ç©ºé—´æ¨¡å‹
â”œâ”€â”€ train/                            # ğŸ¯ è®­ç»ƒæ¡†æ¶
â”‚   â”œâ”€â”€ engine.py                    # è®­ç»ƒå¼•æ“+DeepSpeedé›†æˆ
â”‚   â”œâ”€â”€ data.py                      # æ•°æ®åŠ è½½+è¯¾ç¨‹å­¦ä¹ 
â”‚   â”œâ”€â”€ lazy_data.py                 # å¤§æ–‡ä»¶æ‡’åŠ è½½
â”‚   â”œâ”€â”€ streaming_data.py            # HFæµå¼æ•°æ®
â”‚   â”œâ”€â”€ losses.py                    # å¤åˆæŸå¤±å‡½æ•°
â”‚   â””â”€â”€ monitor_gate.py              # é—¨æ§ç›‘æ§+è£å‰ªè®¡åˆ’
â”œâ”€â”€ deploy/                           # ğŸš€ éƒ¨ç½²å·¥å…·
â”‚   â”œâ”€â”€ export_pruned.py             # æ¨¡å‹è£å‰ªå¯¼å‡º
â”‚   â””â”€â”€ runtime_stub.py              # æ¨ç†è¿è¡Œæ—¶
â”œâ”€â”€ docs/                             # ğŸ“š æŠ€æœ¯æ–‡æ¡£
â”œâ”€â”€ scripts/                          # ğŸ“œ è¿è¡Œè„šæœ¬
â””â”€â”€ environment.yml                   # ğŸ Condaç¯å¢ƒ
```

## ğŸ”¥ æ¶æ„è®¾è®¡å›¾

ä»¥ä¸‹æ˜¯FusionTreeçš„æ ¸å¿ƒæ¶æ„å›¾ï¼š

```mermaid
graph TB
    subgraph "FusionTree æ··åˆæ¶æ„è¯­è¨€æ¨¡å‹"
        A[è¾“å…¥Tokenåºåˆ—] --> B[è¯åµŒå…¥å±‚]
        B --> C[å…±äº«RoPEç¼“å­˜]
        
        subgraph "HybridBlock Stack (18å±‚)"
            D[è¾“å…¥å±‚å½’ä¸€åŒ–] --> E[å…±äº«SRTEç¼–ç ]
            E --> F[åˆ†æ”¯è¾“å…¥å‡†å¤‡]
            
            subgraph "å¹¶è¡ŒåŒåˆ†æ”¯"
                F --> G[Mambaåˆ†æ”¯<br/>+æ—¶é—´ç¼–ç ]
                F --> H[Attentionåˆ†æ”¯<br/>+RoPEç¼–ç ]
                
                subgraph "Attentionåˆ†æ”¯è¯¦æƒ…"
                    H --> I[QKVæŠ•å½±]
                    I --> J[å±€éƒ¨å¤´<br/>æ»‘çª—256]
                    I --> K[å…¨å±€å¤´Ã—2<br/>å®Œæ•´åºåˆ—]
                    J --> L[SDPAå—åŒ–è®¡ç®—]
                    K --> M[SDPAå…¨å±€è®¡ç®—]
                    L --> N[åˆå¹¶å±€éƒ¨+å…¨å±€]
                    M --> N
                end
                
                subgraph "Mambaåˆ†æ”¯è¯¦æƒ…"
                    G --> O[SSMæ‰«æ]
                    O --> P[çŠ¶æ€æ›´æ–°]
                    P --> Q[è¾“å‡ºæŠ•å½±]
                end
            end
            
            N --> R[ç‰¹å¾å¯¹é½MLP]
            Q --> R
            R --> S[ä½ç§©é—¨æ§<br/>é€é€šé“æƒé‡]
            S --> T[é—¨æ§èåˆ]
            T --> U[èåˆæŠ•å½±]
            U --> V[å°å‹MLP 2H]
            V --> W[æ®‹å·®è¿æ¥+å±‚å½’ä¸€åŒ–]
        end
        
        W --> X[æœ€ç»ˆå±‚å½’ä¸€åŒ–]
        X --> Y[è¯­è¨€å»ºæ¨¡å¤´]
        Y --> Z[è¾“å‡ºLogits]
        
        subgraph "è®­ç»ƒä¼˜åŒ–"
            AA[æ¢¯åº¦æ£€æŸ¥ç‚¹<br/>Mamba+Attention]
            BB[SDPAè‡ªåŠ¨åç«¯<br/>FlashAttention2]
            CC[DeepSpeed ZeRO-2<br/>å‚æ•°åˆ†ç‰‡]
            DD[é—¨æ§æ­£åˆ™æŸå¤±<br/>è´Ÿè½½å‡è¡¡+ç†µæ­£åˆ™]
        end
        
        subgraph "ç›‘æ§ä¸è£å‰ª"
            EE[é—¨æ§ç»Ÿè®¡æ”¶é›†] --> FF[è£å‰ªè®¡åˆ’ç”Ÿæˆ]
            FF --> GG[é™æ€æ¨¡å‹å¯¼å‡º]
        end
    end
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <repository_url>
cd FusionTree

# åˆ›å»ºCondaç¯å¢ƒ
conda env create -f environment.yml
conda activate fusiontree-gpu

# æˆ–ä½¿ç”¨è„šæœ¬ä¸€é”®é…ç½®
source env_setup.sh
```

### 2. æ•°æ®å‡†å¤‡

```bash
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p data/train data/eval

# Wikipediaæ•°æ®ç¤ºä¾‹
echo '{"text": "Wikipediaæ˜¯ä¸€ä¸ªè‡ªç”±å†…å®¹ã€å…¬å¼€ç¼–è¾‘ä¸”å¤šè¯­è¨€çš„ç½‘ç»œç™¾ç§‘å…¨ä¹¦..."}' > data/train/wiki_sample.jsonl

# æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
# - JSONL: {"text": "content"}
# - é€šé…ç¬¦è·¯å¾„: data/train/*.jsonl
# - HuggingFaceæ•°æ®é›†æµå¼åŠ è½½
```

### 3. æ¨¡å‹è®­ç»ƒ

```bash
# ğŸ¯ æ¨èé…ç½®ï¼šWikipediaæ•°æ® (8Ã—A6000)
torchrun --nproc_per_node=8 train/engine.py \
  --config configs/pretrain_wikipedia.yaml \
  --distributed

# ğŸ”§ å°å‹æ¨¡å‹è°ƒè¯•
python train/engine.py --config configs/pretrain_optimized.yaml

# ğŸ“Š è®­ç»ƒç›‘æ§
# - Wandb: è®­ç»ƒæ›²çº¿ã€é—¨æ§ç»Ÿè®¡
# - æ—¥å¿—: lossä¸‹é™ã€gate_meanå˜åŒ–
# - æ˜¾å­˜: æ¯å¡<35GB (å……åˆ†åˆ©ç”¨48GB)
```

### 4. æ¨¡å‹è¯„ä¼°ä¸éƒ¨ç½²

```bash
# æ¨¡å‹è¯„ä¼°
python eval/eval_llm.py --model_path checkpoints/step_5000

# è£å‰ªæ¨¡å‹å¯¼å‡º
python deploy/export_pruned.py \
  --model_path checkpoints/step_5000 \
  --prune_plan checkpoints/prune_plan.json \
  --output_dir deployed_models/
```

## ğŸ“Š æ ¸å¿ƒç®—æ³•è¯¦è§£

### HybridBlock å‰å‘ä¼ æ’­

```python
def forward(self, hidden_states, attention_mask=None, training=True):
    # 1. è¾“å…¥å½’ä¸€åŒ–
    normalized_input = self.ln_input(hidden_states)
    
    # 2. ç»Ÿä¸€æ—¶é—´ç¼–ç 
    time_encoding = self.srte(seq_len).to(hidden_states.dtype)
    
    # 3. åˆ†æ”¯è¾“å…¥å‡†å¤‡
    mamba_input = normalized_input + time_encoding  # Mambaç”¨SRTE
    attn_input = normalized_input                   # Attentionç”¨RoPE
    
    # 4. å¹¶è¡ŒåŒåˆ†æ”¯è®¡ç®— (æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹)
    if self.gradient_checkpointing and training:
        h_mamba = checkpoint(lambda x: self.mamba(x)[0], mamba_input)
        h_attn = checkpoint(lambda x: self.attention(x)[0], attn_input)
    else:
        h_mamba, _ = self.mamba(mamba_input, state=mamba_state)
        h_attn, _ = self.attention(attn_input, attention_mask=attention_mask)
    
    # 5. ç‰¹å¾å¯¹é½ä¸é—¨æ§èåˆ
    aligned_features = self.alignment(h_mamba, h_attn)
    gate_weights = self.gate(aligned_features)  # é€é€šé“æƒé‡
    fused_features = gate_weights * h_mamba + (1 - gate_weights) * h_attn
    
    # 6. è¾“å‡ºæŠ•å½±ä¸æ®‹å·®
    projected = self.fusion_proj(fused_features)
    mlp_output = self.small_mlp(aligned_features)
    output = hidden_states + projected + mlp_output
    
    return self.ln_output(output)
```

### SDPAä¼˜åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶

```python
# å…¨å±€æ³¨æ„åŠ›ï¼šä½¿ç”¨PyTorch 2.4 SDPA
def global_attention(q, k, v, attention_mask=None):
    attn_mask = attention_mask[:, None, None, :] if attention_mask else None
    return F.scaled_dot_product_attention(
        q, k, v, 
        attn_mask=attn_mask, 
        is_causal=True  # è‡ªåŠ¨å› æœæ©ç ï¼Œæ›¿ä»£æ‰‹å·¥å®ç°
    )

# æ»‘çª—æ³¨æ„åŠ›ï¼šå—åŒ–+SDPA
def sliding_window_attention(q, k, v, window_size=256):
    for start_idx in range(0, seq_len, block_size):
        # è®¡ç®—æ»‘çª—èŒƒå›´
        k_start = max(0, start_idx - window_size // 2)
        k_end = min(seq_len, end_idx + window_size // 2)
        
        # å—å†…SDPAè®¡ç®—
        output_block = F.scaled_dot_product_attention(
            q_block, k_block, v_block, is_causal=True
        )
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–ç­–ç•¥

| ä¼˜åŒ–æŠ€æœ¯ | æ˜¾å­˜èŠ‚çœ | è¯´æ˜ |
|---------|---------|------|
| **å…±äº«RoPEç¼“å­˜** | ~1GB | 18å±‚å…±äº«cos/sinè¡¨ï¼ŒåŠ¨æ€æ‰©å®¹ |
| **é€‰æ‹©æ€§Checkpoint** | 30-50% | ä»…å¯¹Mamba/Attentioné‡è®¡ç®— |
| **SDPAåç«¯** | 20-30% | è‡ªåŠ¨é€‰æ‹©FlashAttention2å†…æ ¸ |
| **ZeRO-2åˆ†ç‰‡** | 2-8x | å‚æ•°åˆ†ç‰‡ï¼Œé€šä¿¡å¼€é”€æ›´å° |

### 8Ã—A6000 (48GB/å¡) æ¨èé…ç½®

```yaml
# configs/pretrain_wikipedia.yaml
training:
  batch_size: 4                    # æ¯å¡micro-batch
  gradient_accumulation_steps: 2   # æ€»batch=4Ã—8Ã—2Ã—1024=65k tokens
  max_seq_length: 1024             # å®é™…è®­ç»ƒé•¿åº¦
  gradient_checkpointing: true     # å¯ç”¨é€‰æ‹©æ€§é‡è®¡ç®—

model:
  hidden_size: 1536               # å¹³è¡¡ç²¾åº¦ä¸æ•ˆç‡
  num_layers: 18                  # é€‚ä¸­æ·±åº¦
  window_size: 256                # å±€éƒ¨çª—å£å¤§å°
  global_heads: 2                 # ä¿æŒé•¿ç¨‹å»ºæ¨¡
  max_position_embeddings: 8192   # RoPEç¼“å­˜å¤§å°

system:
  zero_stage: 2                   # ZeRO-2è·å¾—æ›´å¥½åå
  offload_optimizer: false        # å…³é—­offloadé¿å…I/Oç“¶é¢ˆ
  offload_params: false
```

### é¢„æœŸæ€§èƒ½æŒ‡æ ‡

**è®­ç»ƒé˜¶æ®µ** (5000æ­¥ï¼ŒWikipediaæ•°æ®)ï¼š
- **LM Loss**: 7.5 â†’ 5.5-6.2
- **é—¨æ§ç»Ÿè®¡**: gate_mean 0.45-0.55ï¼Œstd 0.15â†’0.08-0.12
- **æ˜¾å­˜ä½¿ç”¨**: <35GB/å¡ (48GBå¡æœ‰å……åˆ†ä½™é‡)
- **ååæå‡**: æ¯”ZeRO-3 + micro-batch=2å¿«20-30%

**æ¨ç†é˜¶æ®µ** (è£å‰ªå)ï¼š
- **Prefill**: â‰¥90% åŸºçº¿åå
- **Decode**: +37% é€Ÿåº¦æå‡
- **æ˜¾å­˜èŠ‚çœ**: 22% å‡å°‘

## ğŸ¯ è®­ç»ƒç›‘æ§

### å…³é”®è§‚æµ‹æŒ‡æ ‡

```python
# 1. æŸå¤±æ›²çº¿å¥åº·æ£€æŸ¥
if step % 100 == 0:
    print(f"Step {step}: LM_loss={lm_loss:.4f}, Gate_mean={gate_mean:.3f}")

# 2. é—¨æ§åˆ†å¸ƒæ¼”åŒ–
# - æ—©æœŸ: gate_mean~0.5, é«˜std
# - ä¸­æœŸ: é€æ­¥åˆ†åŒ–ï¼Œå±‚é—´å·®å¼‚æ˜¾ç°
# - åæœŸ: ç¨³å®šåå‘ï¼ŒæŸäº›å±‚åMamba/Attention

# 3. æ­£åˆ™åŒ–æ•ˆæœ (ä¿®å¤detachå)
# - load_balance_loss: çº¦æŸgate_meanæ¥è¿‘ç›®æ ‡å€¼
# - entropy_loss: é˜²æ­¢é—¨æ§è¿‡æ—©æ”¶æ•›åˆ°æå€¼
```

### æ•…éšœæ’æŸ¥

| é—®é¢˜ç°è±¡ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|---------|---------|
| é—¨æ§é•¿æœŸè´´0.5ä¸å˜ | æ­£åˆ™æƒé‡è¿‡å° | è°ƒé«˜`entropy_reg_coeff` |
| é—¨æ§å¿«é€Ÿæ”¶æ•›åˆ°æå€¼ | è´Ÿè½½å‡è¡¡è¿‡å¼º | è°ƒä½`load_balance_coeff` |
| Lossä¸ä¸‹é™ | æ•°æ®/æ¢¯åº¦é—®é¢˜ | æ£€æŸ¥detachä¿®å¤ |
| NCCLè¶…æ—¶ | ä¿å­˜æ—¶èšåˆ | ç¡®è®¤`gather_16bit_weights_on_model_save: false` |

## ğŸ”¬ æŠ€æœ¯åŸç†

### Mamba vs Attention äº’è¡¥æ€§

```
é•¿ç¨‹ä¾èµ–å»ºæ¨¡:
â”œâ”€â”€ Mambaåˆ†æ”¯: O(L)å¤æ‚åº¦ï¼Œæ“…é•¿åºåˆ—çº§è¯­ä¹‰
â”‚   â”œâ”€â”€ çŠ¶æ€ç©ºé—´å»ºæ¨¡
â”‚   â”œâ”€â”€ é€‰æ‹©æ€§æœºåˆ¶
â”‚   â””â”€â”€ å¹¶è¡Œæ‰«æè®­ç»ƒ
â””â”€â”€ Attentionåˆ†æ”¯: O(LÂ²)â†’O(LÃ—W)ï¼Œæ“…é•¿å±€éƒ¨ç»†èŠ‚
    â”œâ”€â”€ æ»‘çª—å±€éƒ¨æ³¨æ„åŠ› (å¤§éƒ¨åˆ†å¤´)
    â”œâ”€â”€ å…¨å±€æ³¨æ„åŠ› (å°‘é‡å¤´)
    â””â”€â”€ SDPAå†…æ ¸ä¼˜åŒ–
```

### é—¨æ§èåˆæœºåˆ¶

```python
# ä½ç§©é—¨æ§: Hâ†’râ†’Hï¼Œå‡å°‘å‚æ•°é‡
class GateLowRank(nn.Module):
    def __init__(self, hidden_size, rank=96):
        self.u = nn.Linear(hidden_size, rank, bias=False)
        self.d = nn.Linear(rank, hidden_size, bias=False)
    
    def forward(self, z):
        return torch.sigmoid(self.d(torch.tanh(self.u(z))))

# é€é€šé“èåˆ: ç»†ç²’åº¦æ§åˆ¶
fused = gate_weights * h_mamba + (1 - gate_weights) * h_attn
```

## ğŸ“š ç›¸å…³å·¥ä½œ

- **[Mamba](https://arxiv.org/abs/2312.00752)**: çŠ¶æ€ç©ºé—´æ¨¡å‹çš„çªç ´æ€§è¿›å±•
- **[FlashAttention](https://arxiv.org/abs/2205.14135)**: é«˜æ•ˆæ³¨æ„åŠ›å®ç°
- **[RoPE](https://arxiv.org/abs/2104.09864)**: æ—‹è½¬ä½ç½®ç¼–ç 
- **[DeepSpeed](https://github.com/microsoft/DeepSpeed)**: å¤§æ¨¡å‹è®­ç»ƒä¼˜åŒ–

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. **ä»£ç é£æ ¼**: éµå¾ªPEP 8ï¼Œä½¿ç”¨type hints
2. **æµ‹è¯•**: æ–°åŠŸèƒ½éœ€è¦å•å…ƒæµ‹è¯•
3. **æ–‡æ¡£**: é‡è¦å‡½æ•°æ·»åŠ docstring
4. **æ€§èƒ½**: å…³æ³¨æ˜¾å­˜å’Œé€Ÿåº¦ä¼˜åŒ–

```bash
# å¼€å‘ç¯å¢ƒé…ç½®
git clone <your-fork>
cd FusionTree
pre-commit install  # ä»£ç æ ¼å¼æ£€æŸ¥
pytest tests/       # è¿è¡Œæµ‹è¯•å¥—ä»¶
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ä¸ºFusionTreeæä¾›çš„æŠ€æœ¯åŸºç¡€ï¼š
- **Mamba**: é©å‘½æ€§çš„çŠ¶æ€ç©ºé—´æ¨¡å‹æ¶æ„
- **PyTorch**: æ·±åº¦å­¦ä¹ æ¡†æ¶å’ŒSDPAä¼˜åŒ–
- **DeepSpeed**: å¤§æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒè§£å†³æ–¹æ¡ˆ
- **FlashAttention**: é«˜æ•ˆæ³¨æ„åŠ›ç®—æ³•

---

â­ **å¦‚æœFusionTreeå¯¹æ‚¨çš„ç ”ç©¶æˆ–é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªStarï¼**

ğŸ“§ **è”ç³»æˆ‘ä»¬**: [Issues](../../issues) | [Discussions](../../discussions) 